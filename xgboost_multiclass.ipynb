{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "epxCprGQOqh6",
        "outputId": "68aeb987-e23e-4dae-9ed3-f10b83130e79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounting Google Drive...\n",
            "Mounted at /content/drive\n",
            "Google Drive mounted successfully.\n",
            "\n",
            "Directory exists: /content/drive/My Drive/CICIDS2017_improved\n",
            "Files in directory:\n",
            "  - monday.csv\n",
            "  - tuesday.csv\n",
            "  - wednesday.csv\n",
            "  - thursday.csv\n",
            "  - friday.csv\n",
            "  - combined_dataset.csv\n",
            "  - balanced_multiclass_10k.csv\n",
            "  - train.csv\n",
            "  - metrics_by_class.png\n",
            "  - class_metrics.csv\n",
            "  - prediction_results.csv\n",
            "  - confusion_matrix.csv\n",
            "  - confusion_metrics_by_class.png\n",
            "  - fixed_train.csv\n",
            "  - fixed_test.csv\n",
            "  - enhanced_train.csv\n",
            "  - plots_multiclass\n",
            "  - multiclass_metrics.csv\n",
            "Loading combined dataset from /content/drive/My Drive/CICIDS2017_improved/combined_dataset.csv...\n",
            "Loaded dataset with comma delimiter. Shape: (250000, 91)\n",
            "\n",
            "Splitting dataset into training (70%) and testing (30%) sets...\n",
            "\n",
            "Full dataset class distribution:\n",
            "  BENIGN: 197771 (79.11%)\n",
            "  DoS Hulk: 15917 (6.37%)\n",
            "  Portscan: 14170 (5.67%)\n",
            "  Infiltration - Portscan: 9886 (3.95%)\n",
            "  DDoS: 8585 (3.43%)\n",
            "  DoS GoldenEye: 733 (0.29%)\n",
            "  FTP-Patator: 618 (0.25%)\n",
            "  SSH-Patator: 446 (0.18%)\n",
            "  DoS Slowloris: 370 (0.15%)\n",
            "  DoS Slowhttptest - Attempted: 351 (0.14%)\n",
            "  Botnet - Attempted: 346 (0.14%)\n",
            "  DoS Slowloris - Attempted: 189 (0.08%)\n",
            "  DoS Slowhttptest: 186 (0.07%)\n",
            "  Web Attack - Brute Force - Attempted: 176 (0.07%)\n",
            "  Web Attack - XSS - Attempted: 92 (0.04%)\n",
            "  DoS Hulk - Attempted: 63 (0.03%)\n",
            "  Botnet: 57 (0.02%)\n",
            "  Web Attack - Brute Force: 9 (0.00%)\n",
            "  DoS GoldenEye - Attempted: 8 (0.00%)\n",
            "  SSH-Patator - Attempted: 7 (0.00%)\n",
            "  Infiltration - Attempted: 6 (0.00%)\n",
            "  Web Attack - XSS: 4 (0.00%)\n",
            "  Infiltration: 4 (0.00%)\n",
            "  Web Attack - SQL Injection: 3 (0.00%)\n",
            "  FTP-Patator - Attempted: 2 (0.00%)\n",
            "  Heartbleed: 1 (0.00%)\n",
            "Training set: 175000 samples\n",
            "Test set: 75000 samples\n",
            "\n",
            "Training set class distribution:\n",
            "  BENIGN: 138591 (79.19%)\n",
            "  DoS Hulk: 11083 (6.33%)\n",
            "  Portscan: 9769 (5.58%)\n",
            "  Infiltration - Portscan: 6958 (3.98%)\n",
            "  DDoS: 6011 (3.43%)\n",
            "  DoS GoldenEye: 522 (0.30%)\n",
            "  FTP-Patator: 440 (0.25%)\n",
            "  SSH-Patator: 308 (0.18%)\n",
            "  DoS Slowloris: 256 (0.15%)\n",
            "  DoS Slowhttptest - Attempted: 256 (0.15%)\n",
            "  Botnet - Attempted: 247 (0.14%)\n",
            "  DoS Slowloris - Attempted: 130 (0.07%)\n",
            "  DoS Slowhttptest: 124 (0.07%)\n",
            "  Web Attack - Brute Force - Attempted: 115 (0.07%)\n",
            "  Web Attack - XSS - Attempted: 69 (0.04%)\n",
            "  DoS Hulk - Attempted: 44 (0.03%)\n",
            "  Botnet: 40 (0.02%)\n",
            "  Web Attack - Brute Force: 7 (0.00%)\n",
            "  SSH-Patator - Attempted: 7 (0.00%)\n",
            "  DoS GoldenEye - Attempted: 6 (0.00%)\n",
            "  Infiltration - Attempted: 5 (0.00%)\n",
            "  Web Attack - XSS: 4 (0.00%)\n",
            "  Infiltration: 3 (0.00%)\n",
            "  FTP-Patator - Attempted: 2 (0.00%)\n",
            "  Web Attack - SQL Injection: 2 (0.00%)\n",
            "  Heartbleed: 1 (0.00%)\n",
            "\n",
            "Test set class distribution:\n",
            "  BENIGN: 59180 (78.91%)\n",
            "  DoS Hulk: 4834 (6.45%)\n",
            "  Portscan: 4401 (5.87%)\n",
            "  Infiltration - Portscan: 2928 (3.90%)\n",
            "  DDoS: 2574 (3.43%)\n",
            "  DoS GoldenEye: 211 (0.28%)\n",
            "  FTP-Patator: 178 (0.24%)\n",
            "  SSH-Patator: 138 (0.18%)\n",
            "  DoS Slowloris: 114 (0.15%)\n",
            "  Botnet - Attempted: 99 (0.13%)\n",
            "  DoS Slowhttptest - Attempted: 95 (0.13%)\n",
            "  DoS Slowhttptest: 62 (0.08%)\n",
            "  Web Attack - Brute Force - Attempted: 61 (0.08%)\n",
            "  DoS Slowloris - Attempted: 59 (0.08%)\n",
            "  Web Attack - XSS - Attempted: 23 (0.03%)\n",
            "  DoS Hulk - Attempted: 19 (0.03%)\n",
            "  Botnet: 17 (0.02%)\n",
            "  Web Attack - Brute Force: 2 (0.00%)\n",
            "  DoS GoldenEye - Attempted: 2 (0.00%)\n",
            "  Infiltration: 1 (0.00%)\n",
            "  Web Attack - SQL Injection: 1 (0.00%)\n",
            "  Infiltration - Attempted: 1 (0.00%)\n",
            "\n",
            "Preprocessing training data...\n",
            "\n",
            "Preprocessing data:\n",
            "Initial columns: ['id', 'Flow ID', 'Src IP', 'Src Port', 'Dst IP']... (total: 91)\n",
            "Label distribution: {'BENIGN': 138591, 'DoS Hulk': 11083, 'Portscan': 9769, 'Infiltration - Portscan': 6958, 'DDoS': 6011, 'DoS GoldenEye': 522, 'FTP-Patator': 440, 'SSH-Patator': 308, 'DoS Slowloris': 256, 'DoS Slowhttptest - Attempted': 256, 'Botnet - Attempted': 247, 'DoS Slowloris - Attempted': 130, 'DoS Slowhttptest': 124, 'Web Attack - Brute Force - Attempted': 115, 'Web Attack - XSS - Attempted': 69, 'DoS Hulk - Attempted': 44, 'Botnet': 40, 'Web Attack - Brute Force': 7, 'SSH-Patator - Attempted': 7, 'DoS GoldenEye - Attempted': 6, 'Infiltration - Attempted': 5, 'Web Attack - XSS': 4, 'Infiltration': 3, 'FTP-Patator - Attempted': 2, 'Web Attack - SQL Injection': 2, 'Heartbleed': 1}\n",
            "\n",
            "Label encoding mapping:\n",
            "  0: BENIGN\n",
            "  1: Botnet\n",
            "  2: Botnet - Attempted\n",
            "  3: DDoS\n",
            "  4: DoS GoldenEye\n",
            "  5: DoS GoldenEye - Attempted\n",
            "  6: DoS Hulk\n",
            "  7: DoS Hulk - Attempted\n",
            "  8: DoS Slowhttptest\n",
            "  9: DoS Slowhttptest - Attempted\n",
            "  10: DoS Slowloris\n",
            "  11: DoS Slowloris - Attempted\n",
            "  12: FTP-Patator\n",
            "  13: FTP-Patator - Attempted\n",
            "  14: Heartbleed\n",
            "  15: Infiltration\n",
            "  16: Infiltration - Attempted\n",
            "  17: Infiltration - Portscan\n",
            "  18: Portscan\n",
            "  19: SSH-Patator\n",
            "  20: SSH-Patator - Attempted\n",
            "  21: Web Attack - Brute Force\n",
            "  22: Web Attack - Brute Force - Attempted\n",
            "  23: Web Attack - SQL Injection\n",
            "  24: Web Attack - XSS\n",
            "  25: Web Attack - XSS - Attempted\n",
            "Number of numeric columns after conversion: 82\n",
            "NaN values before handling: 0\n",
            "NaN values after handling: 0\n",
            "Preprocessed dataset shape: (175000, 84)\n",
            "Number of unique classes: 26\n",
            "\n",
            "Preprocessing test data...\n",
            "\n",
            "Preprocessing data:\n",
            "Initial columns: ['id', 'Flow ID', 'Src IP', 'Src Port', 'Dst IP']... (total: 91)\n",
            "Label distribution: {'BENIGN': 59180, 'DoS Hulk': 4834, 'Portscan': 4401, 'Infiltration - Portscan': 2928, 'DDoS': 2574, 'DoS GoldenEye': 211, 'FTP-Patator': 178, 'SSH-Patator': 138, 'DoS Slowloris': 114, 'Botnet - Attempted': 99, 'DoS Slowhttptest - Attempted': 95, 'DoS Slowhttptest': 62, 'Web Attack - Brute Force - Attempted': 61, 'DoS Slowloris - Attempted': 59, 'Web Attack - XSS - Attempted': 23, 'DoS Hulk - Attempted': 19, 'Botnet': 17, 'Web Attack - Brute Force': 2, 'DoS GoldenEye - Attempted': 2, 'Infiltration': 1, 'Web Attack - SQL Injection': 1, 'Infiltration - Attempted': 1}\n",
            "Number of numeric columns after conversion: 82\n",
            "NaN values before handling: 0\n",
            "NaN values after handling: 0\n",
            "Preprocessed dataset shape: (75000, 84)\n",
            "Number of unique classes: 22\n",
            "\n",
            "Training features shape: (175000, 82)\n",
            "Training target shape: (175000,)\n",
            "Test features shape: (75000, 82)\n",
            "Test target shape: (75000,)\n",
            "\n",
            "Handling class imbalance with SMOTE:\n",
            "Class distribution before SMOTE:\n",
            "  Class 0: 138591 samples (79.19%)\n",
            "  Class 1: 40 samples (0.02%)\n",
            "  Class 2: 247 samples (0.14%)\n",
            "  Class 3: 6011 samples (3.43%)\n",
            "  Class 4: 522 samples (0.30%)\n",
            "  Class 5: 6 samples (0.00%)\n",
            "  Class 6: 11083 samples (6.33%)\n",
            "  Class 7: 44 samples (0.03%)\n",
            "  Class 8: 124 samples (0.07%)\n",
            "  Class 9: 256 samples (0.15%)\n",
            "  Class 10: 256 samples (0.15%)\n",
            "  Class 11: 130 samples (0.07%)\n",
            "  Class 12: 440 samples (0.25%)\n",
            "  Class 13: 2 samples (0.00%)\n",
            "  Class 14: 1 samples (0.00%)\n",
            "  Class 15: 3 samples (0.00%)\n",
            "  Class 16: 5 samples (0.00%)\n",
            "  Class 17: 6958 samples (3.98%)\n",
            "  Class 18: 9769 samples (5.58%)\n",
            "  Class 19: 308 samples (0.18%)\n",
            "  Class 20: 7 samples (0.00%)\n",
            "  Class 21: 7 samples (0.00%)\n",
            "  Class 22: 115 samples (0.07%)\n",
            "  Class 23: 2 samples (0.00%)\n",
            "  Class 24: 4 samples (0.00%)\n",
            "  Class 25: 69 samples (0.04%)\n",
            "Warning: Minimum samples per class (1) is too small for SMOTE.\n",
            "Using original imbalanced data\n",
            "\n",
            "Training XGBoost model for multiclass classification...\n",
            "Number of classes: 26\n",
            "Training without validation set...\n",
            "\n",
            "Evaluating multiclass model...\n",
            "\n",
            "Model               Accuracy    F1 Score    False PostiveFalse NegativeDetection   \n",
            "XGBoost, Dataset A  0.994306670.99428752427         427         0.99430667\n",
            "Prediction time: 39.76 μs/sample\n",
            "\n",
            "Classification Report:\n",
            "                                      precision    recall  f1-score   support\n",
            "\n",
            "                              BENIGN       1.00      1.00      1.00     59180\n",
            "                              Botnet       0.94      0.94      0.94        17\n",
            "                  Botnet - Attempted       1.00      1.00      1.00        99\n",
            "                                DDoS       1.00      1.00      1.00      2574\n",
            "                       DoS GoldenEye       1.00      1.00      1.00       211\n",
            "           DoS GoldenEye - Attempted       1.00      1.00      1.00         2\n",
            "                            DoS Hulk       1.00      1.00      1.00      4834\n",
            "                DoS Hulk - Attempted       1.00      1.00      1.00        19\n",
            "                    DoS Slowhttptest       1.00      1.00      1.00        62\n",
            "        DoS Slowhttptest - Attempted       1.00      1.00      1.00        95\n",
            "                       DoS Slowloris       1.00      0.98      0.99       114\n",
            "           DoS Slowloris - Attempted       1.00      1.00      1.00        59\n",
            "                         FTP-Patator       1.00      0.99      0.99       178\n",
            "                        Infiltration       0.00      0.00      0.00         1\n",
            "            Infiltration - Attempted       1.00      1.00      1.00         1\n",
            "             Infiltration - Portscan       0.95      0.92      0.94      2928\n",
            "                            Portscan       0.95      0.97      0.96      4401\n",
            "                         SSH-Patator       1.00      1.00      1.00       138\n",
            "            Web Attack - Brute Force       1.00      1.00      1.00         2\n",
            "Web Attack - Brute Force - Attempted       0.69      0.62      0.66        61\n",
            "          Web Attack - SQL Injection       0.00      0.00      0.00         1\n",
            "        Web Attack - XSS - Attempted       0.21      0.26      0.24        23\n",
            "\n",
            "                            accuracy                           0.99     75000\n",
            "                           macro avg       0.85      0.85      0.85     75000\n",
            "                        weighted avg       0.99      0.99      0.99     75000\n",
            "\n",
            "\n",
            "Performance Metrics:\n",
            "  Accuracy: 0.994307\n",
            "  Macro F1 Score: 0.850353\n",
            "  Weighted F1 Score: 0.994288\n",
            "  Prediction Time (μs/sample): 39.763597\n",
            "  False Positives: 427.000000\n",
            "  False Negatives: 427.000000\n",
            "  Detection: 0.994307\n",
            "Confusion matrix saved as 'confusion_matrix_multiclass.png'\n",
            "Feature importance plot saved as 'feature_importance_multiclass.png'\n",
            "\n",
            "Creating 10k sample subset for additional evaluation...\n",
            "\n",
            "Evaluating on 10k subset:\n",
            "Model               Accuracy    F1 Score    False PostiveFalse NegativeDetection   \n",
            "XGBoost, Dataset A, 10k subsample0.994000000.9939230360          60          0.99400000\n",
            "Multiclass ROC curves saved as 'roc_curves_multiclass.png'\n",
            "\n",
            "Model saved as 'xgboost_multiclass_model.json'\n",
            "\n",
            "Performance Comparison:\n",
            "Baseline F1 Score (from paper): ~70%\n",
            "Our Model's Weighted F1 Score: 99.43%\n",
            "Improvement: 42.04%\n",
            "\n",
            "Analysis complete!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x1000 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#This code is done using a random split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import time\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, roc_curve, f1_score, confusion_matrix, classification_report\n",
        "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc\n",
        "import xgboost as xgb\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def calculate_detailed_metrics(y_true, y_pred, model_name):\n",
        "    \"\"\"\n",
        "    Calculate detailed metrics for model evaluation.\n",
        "\n",
        "    Args:\n",
        "        y_true: True labels\n",
        "        y_pred: Predicted labels\n",
        "        model_name: Name of the model for display\n",
        "\n",
        "    Returns:\n",
        "        Dictionary of metrics\n",
        "    \"\"\"\n",
        "    # Basic metrics\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "\n",
        "    # Confusion matrix to get FP and FN\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    # For multi-class, calculate total false positives and false negatives\n",
        "    false_positives = 0\n",
        "    false_negatives = 0\n",
        "\n",
        "    # Loop through each class\n",
        "    for i in range(len(cm)):\n",
        "        # False positives are sum of column i minus value at position (i,i)\n",
        "        false_positives += sum(cm[:,i]) - cm[i,i]\n",
        "        # False negatives are sum of row i minus value at position (i,i)\n",
        "        false_negatives += sum(cm[i,:]) - cm[i,i]\n",
        "\n",
        "    # Calculate detection rate (True Positives / (True Positives + False Negatives))\n",
        "    # For multiclass, detection is macro-averaged recall\n",
        "    detection = np.sum(np.diag(cm)) / np.sum(cm)\n",
        "\n",
        "    # Print results in tabular format\n",
        "    print(f\"{model_name:<20}{accuracy:.8f}{f1:.8f}{false_positives:<12}{false_negatives:<12}{detection:.8f}\")\n",
        "\n",
        "    # Return metrics dict\n",
        "    return {\n",
        "        'Accuracy': accuracy,\n",
        "        'F1 Score': f1,\n",
        "        'False Positives': false_positives,\n",
        "        'False Negatives': false_negatives,\n",
        "        'Detection': detection\n",
        "    }\n",
        "\n",
        "def load_data(file_path):\n",
        "    \"\"\"\n",
        "    Load network flow data from file path, attempting multiple delimiters.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # First try comma delimiter (most common for CICIDS2017)\n",
        "        df = pd.read_csv(file_path, delimiter=',', low_memory=False)\n",
        "        print(f\"Loaded dataset with comma delimiter. Shape: {df.shape}\")\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"Error with comma delimiter: {e}\")\n",
        "\n",
        "        # Try with tab delimiter\n",
        "        try:\n",
        "            df = pd.read_csv(file_path, delimiter='\\t', low_memory=False)\n",
        "\n",
        "            # Check if we ended up with only one column containing all data\n",
        "            if len(df.columns) == 1 and ',' in df.iloc[0, 0]:\n",
        "                print(\"Data loaded as a single column. Trying comma delimiter again...\")\n",
        "\n",
        "                # Try with comma delimiter\n",
        "                df = pd.read_csv(file_path, delimiter=',', low_memory=False)\n",
        "                print(f\"Loaded dataset with comma delimiter. Shape: {df.shape}\")\n",
        "                return df\n",
        "\n",
        "            print(f\"Loaded dataset with tab delimiter. Shape: {df.shape}\")\n",
        "            return df\n",
        "        except Exception as e:\n",
        "            print(f\"Error with tab delimiter: {e}\")\n",
        "\n",
        "            # Try a manual approach\n",
        "            try:\n",
        "                with open(file_path, 'r') as f:\n",
        "                    lines = f.readlines()\n",
        "\n",
        "                # Detect delimiter from first line\n",
        "                first_line = lines[0].strip()\n",
        "                if '\\t' in first_line and ',' in first_line:\n",
        "                    # If both tab and comma exist, use the one that gives more splits\n",
        "                    tab_count = first_line.count('\\t')\n",
        "                    comma_count = first_line.count(',')\n",
        "                    delimiter = '\\t' if tab_count > comma_count else ','\n",
        "                elif '\\t' in first_line:\n",
        "                    delimiter = '\\t'\n",
        "                elif ',' in first_line:\n",
        "                    delimiter = ','\n",
        "                else:\n",
        "                    delimiter = ',' # Default to comma\n",
        "\n",
        "                print(f\"Using manual parsing with delimiter: '{delimiter}'\")\n",
        "\n",
        "                # Parse manually\n",
        "                headers = lines[0].strip().split(delimiter)\n",
        "                data = []\n",
        "\n",
        "                for i in range(1, len(lines)):\n",
        "                    if lines[i].strip():  # Skip empty lines\n",
        "                        row = lines[i].strip().split(delimiter)\n",
        "                        if len(row) == len(headers):\n",
        "                            data.append(row)\n",
        "                        else:\n",
        "                            print(f\"Warning: Line {i+1} has {len(row)} fields, expected {len(headers)}\")\n",
        "\n",
        "                df = pd.DataFrame(data, columns=headers)\n",
        "                print(f\"Manually loaded dataset with shape: {df.shape}\")\n",
        "                return df\n",
        "            except Exception as e:\n",
        "                print(f\"Error with manual parsing: {e}\")\n",
        "                raise\n",
        "\n",
        "def preprocess_data(df, scaler=None, label_encoder=None, fit_scaler=False, fit_encoder=False):\n",
        "    \"\"\"\n",
        "    Preprocess the network flow data for multiclass classification:\n",
        "    - Remove flow identifiers\n",
        "    - Apply min-max normalization\n",
        "    - Encode attack labels\n",
        "\n",
        "    Args:\n",
        "        df: The dataframe to preprocess\n",
        "        scaler: An optional pre-fitted scaler (for test data)\n",
        "        label_encoder: An optional pre-fitted label encoder (for test data)\n",
        "        fit_scaler: Whether to fit the scaler on this data (for train data)\n",
        "        fit_encoder: Whether to fit the label encoder on this data (for train data)\n",
        "\n",
        "    Returns:\n",
        "        Preprocessed dataframe, scaler, and label encoder\n",
        "    \"\"\"\n",
        "    print(\"\\nPreprocessing data:\")\n",
        "    print(f\"Initial columns: {df.columns.tolist()[:5]}... (total: {len(df.columns)})\")\n",
        "\n",
        "    # Make a copy to avoid modifying the original\n",
        "    df_processed = df.copy()\n",
        "\n",
        "    # Identify the label column (usually 'Label' in CICIDS2017)\n",
        "    label_col = None\n",
        "    for possible_label in ['Label', 'label', 'CLASS', 'class', 'Attack_Type']:\n",
        "        if possible_label in df_processed.columns:\n",
        "            label_col = possible_label\n",
        "            break\n",
        "\n",
        "    if label_col is None:\n",
        "        raise ValueError(\"Could not find a label column in the dataset\")\n",
        "\n",
        "    # Check label distribution\n",
        "    label_counts = df_processed[label_col].value_counts()\n",
        "    print(f\"Label distribution: {label_counts.to_dict()}\")\n",
        "\n",
        "    # Keep the original label\n",
        "    original_label = df_processed[label_col].copy()\n",
        "\n",
        "    # Label encode the attack types\n",
        "    if fit_encoder:\n",
        "        if label_encoder is None:\n",
        "            label_encoder = LabelEncoder()\n",
        "            encoded_labels = label_encoder.fit_transform(original_label)\n",
        "        else:\n",
        "            raise ValueError(\"Cannot fit new encoder when fit_encoder=True but encoder is provided\")\n",
        "    else:\n",
        "        if label_encoder is None:\n",
        "            raise ValueError(\"Either provide a fitted label encoder or set fit_encoder=True\")\n",
        "        try:\n",
        "            encoded_labels = label_encoder.transform(original_label)\n",
        "        except:\n",
        "            # Handle unknown labels by mapping them to the most common label\n",
        "            print(\"Warning: Found unknown labels in test data. Mapping to 'BENIGN'\")\n",
        "            unknown_labels = set(original_label) - set(label_encoder.classes_)\n",
        "            print(f\"Unknown labels: {unknown_labels}\")\n",
        "\n",
        "            # Create a temporary series with known labels\n",
        "            temp_labels = original_label.copy()\n",
        "            temp_labels[temp_labels.isin(unknown_labels)] = 'BENIGN'\n",
        "            encoded_labels = label_encoder.transform(temp_labels)\n",
        "\n",
        "    # Add encoded labels column\n",
        "    df_processed['Label_Encoded'] = encoded_labels\n",
        "\n",
        "    # Display mapping\n",
        "    if fit_encoder:\n",
        "        print(\"\\nLabel encoding mapping:\")\n",
        "        for i, label in enumerate(label_encoder.classes_):\n",
        "            print(f\"  {i}: {label}\")\n",
        "\n",
        "    # Remove flow identifiers and other non-feature columns\n",
        "    columns_to_drop = [\n",
        "        'Flow ID', 'Src IP', 'Src Port', 'Dst IP', 'Dst Port', 'Protocol',\n",
        "        'Timestamp', 'Label', label_col, 'id'\n",
        "    ]\n",
        "\n",
        "    # Only drop columns that exist in the dataframe\n",
        "    columns_to_drop = list(set([col for col in columns_to_drop if col in df_processed.columns]))\n",
        "    df_cleaned = df_processed.drop(columns=columns_to_drop, errors='ignore')\n",
        "\n",
        "    # Convert all columns to numeric, coercing errors to NaN\n",
        "    numeric_cols = []\n",
        "    for col in df_cleaned.columns:\n",
        "        if col != 'Label_Encoded' and col != 'Label_Original':\n",
        "            try:\n",
        "                df_cleaned[col] = pd.to_numeric(df_cleaned[col], errors='coerce')\n",
        "                numeric_cols.append(col)\n",
        "            except Exception as e:\n",
        "                print(f\"Error converting column {col} to numeric: {e}\")\n",
        "                df_cleaned = df_cleaned.drop(columns=[col])\n",
        "\n",
        "    # Display info about numeric columns\n",
        "    print(f\"Number of numeric columns after conversion: {len(numeric_cols)}\")\n",
        "    if len(numeric_cols) == 0:\n",
        "        raise ValueError(\"No numeric columns available after preprocessing\")\n",
        "\n",
        "    # Handle NaN values\n",
        "    print(f\"NaN values before handling: {df_cleaned[numeric_cols].isna().sum().sum()}\")\n",
        "\n",
        "    # Replace infinity values with NaN\n",
        "    df_cleaned = df_cleaned.replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "    # Check for columns with all NaN values\n",
        "    null_cols = [col for col in numeric_cols if df_cleaned[col].isna().all()]\n",
        "    if null_cols:\n",
        "        print(f\"Dropping columns with all NaN values: {null_cols}\")\n",
        "        df_cleaned = df_cleaned.drop(columns=null_cols)\n",
        "        numeric_cols = [col for col in numeric_cols if col not in null_cols]\n",
        "\n",
        "    # Fill remaining NaN values with column means\n",
        "    for col in numeric_cols:\n",
        "        if df_cleaned[col].isna().any():\n",
        "            col_mean = df_cleaned[col].mean()\n",
        "            df_cleaned[col] = df_cleaned[col].fillna(col_mean)\n",
        "\n",
        "    print(f\"NaN values after handling: {df_cleaned[numeric_cols].isna().sum().sum()}\")\n",
        "\n",
        "    # Verify we have data to work with\n",
        "    if len(numeric_cols) == 0:\n",
        "        raise ValueError(\"No numeric columns available after preprocessing\")\n",
        "\n",
        "    # Apply min-max scaling to all numeric columns\n",
        "    features = df_cleaned[numeric_cols]\n",
        "    labels = df_cleaned['Label_Encoded']\n",
        "\n",
        "    # Apply scaling\n",
        "    if scaler is None and fit_scaler:\n",
        "        scaler = MinMaxScaler()\n",
        "        scaled_features = scaler.fit_transform(features)\n",
        "    elif scaler is not None:\n",
        "        scaled_features = scaler.transform(features)\n",
        "    else:\n",
        "        raise ValueError(\"Either provide a fitted scaler or set fit_scaler=True\")\n",
        "\n",
        "    # Create a new dataframe with scaled features\n",
        "    scaled_df = pd.DataFrame(scaled_features, columns=features.columns)\n",
        "\n",
        "    # Add back the label column\n",
        "    scaled_df['Label_Encoded'] = labels.values\n",
        "    scaled_df['Label_Original'] = original_label.values\n",
        "\n",
        "    print(f\"Preprocessed dataset shape: {scaled_df.shape}\")\n",
        "    print(f\"Number of unique classes: {scaled_df['Label_Encoded'].nunique()}\")\n",
        "\n",
        "    if fit_scaler or fit_encoder:\n",
        "        return scaled_df, scaler, label_encoder\n",
        "    else:\n",
        "        return scaled_df\n",
        "\n",
        "def handle_class_imbalance(X, y, sampling_strategy='auto', random_state=42):\n",
        "    \"\"\"\n",
        "    Apply SMOTE to balance the multiclass dataset.\n",
        "\n",
        "    Args:\n",
        "        X: Feature matrix\n",
        "        y: Target labels\n",
        "        sampling_strategy: Strategy for SMOTE\n",
        "        random_state: Random seed\n",
        "\n",
        "    Returns:\n",
        "        Balanced X and y\n",
        "    \"\"\"\n",
        "    print(\"\\nHandling class imbalance with SMOTE:\")\n",
        "\n",
        "    # Check class distribution before SMOTE\n",
        "    class_dist_before = pd.Series(y).value_counts().sort_index()\n",
        "    print(\"Class distribution before SMOTE:\")\n",
        "    for cls, count in class_dist_before.items():\n",
        "        print(f\"  Class {cls}: {count} samples ({count/len(y)*100:.2f}%)\")\n",
        "\n",
        "    # Apply SMOTE if we have enough samples\n",
        "    min_samples_per_class = pd.Series(y).value_counts().min()\n",
        "    if min_samples_per_class < 6:\n",
        "        print(f\"Warning: Minimum samples per class ({min_samples_per_class}) is too small for SMOTE.\")\n",
        "        print(\"Using original imbalanced data\")\n",
        "        return X, y\n",
        "\n",
        "    try:\n",
        "        # Instantiate SMOTE\n",
        "        smote = SMOTE(sampling_strategy=sampling_strategy, random_state=random_state)\n",
        "\n",
        "        # Apply SMOTE\n",
        "        X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "\n",
        "        # Check class distribution after SMOTE\n",
        "        class_dist_after = pd.Series(y_resampled).value_counts().sort_index()\n",
        "        print(\"\\nClass distribution after SMOTE:\")\n",
        "        for cls, count in class_dist_after.items():\n",
        "            print(f\"  Class {cls}: {count} samples ({count/len(y_resampled)*100:.2f}%)\")\n",
        "\n",
        "        print(f\"SMOTE successfully applied. New sample count: {len(y_resampled)} (was {len(y)})\")\n",
        "        return X_resampled, y_resampled\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error applying SMOTE: {e}\")\n",
        "        print(\"Using original imbalanced data\")\n",
        "        return X, y\n",
        "\n",
        "def train_model(X_train, y_train, X_val=None, y_val=None, params=None):\n",
        "    \"\"\"\n",
        "    Train an XGBoost model for multiclass classification\n",
        "\n",
        "    Args:\n",
        "        X_train: Training features\n",
        "        y_train: Training labels\n",
        "        X_val: Validation features\n",
        "        y_val: Validation labels\n",
        "        params: XGBoost parameters\n",
        "\n",
        "    Returns:\n",
        "        Trained XGBoost model\n",
        "    \"\"\"\n",
        "    print(\"\\nTraining XGBoost model for multiclass classification...\")\n",
        "\n",
        "    # Count number of classes\n",
        "    num_classes = len(np.unique(y_train))\n",
        "    print(f\"Number of classes: {num_classes}\")\n",
        "\n",
        "    # Set default parameters if not provided\n",
        "    if params is None:\n",
        "        params = {\n",
        "            'objective': 'multi:softprob',\n",
        "            'num_class': num_classes,\n",
        "            'eta': 0.1,\n",
        "            'max_depth': 6,\n",
        "            'min_child_weight': 1,\n",
        "            'subsample': 0.8,\n",
        "            'colsample_bytree': 0.8,\n",
        "            'tree_method': 'hist',  # For faster training\n",
        "            'eval_metric': 'mlogloss',\n",
        "            'use_label_encoder': False\n",
        "        }\n",
        "\n",
        "    # Create XGBoost model\n",
        "    model = xgb.XGBClassifier(**params)\n",
        "\n",
        "    # Train with validation if provided\n",
        "    if X_val is not None and y_val is not None:\n",
        "        print(\"Training with validation set...\")\n",
        "        eval_set = [(X_train, y_train), (X_val, y_val)]\n",
        "        model.fit(\n",
        "            X_train, y_train,\n",
        "            eval_set=eval_set,\n",
        "            early_stopping_rounds=10,\n",
        "            verbose=True\n",
        "        )\n",
        "    else:\n",
        "        print(\"Training without validation set...\")\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "    return model\n",
        "\n",
        "def evaluate_multiclass_model(model, X_test, y_test, label_encoder=None):\n",
        "    \"\"\"\n",
        "    Evaluate the multiclass model and calculate performance metrics\n",
        "\n",
        "    Args:\n",
        "        model: Trained XGBoost model\n",
        "        X_test: Test features\n",
        "        y_test: Test labels (encoded)\n",
        "        label_encoder: Label encoder to convert indices to original labels\n",
        "\n",
        "    Returns:\n",
        "        Dictionary of performance metrics\n",
        "    \"\"\"\n",
        "    print(\"\\nEvaluating multiclass model...\")\n",
        "\n",
        "    # Print metrics header first\n",
        "    print(f\"\\n{'Model':<20}{'Accuracy':<12}{'F1 Score':<12}{'False Postive':<12}{'False Negative':<12}{'Detection':<12}\")\n",
        "\n",
        "    # Measure prediction time\n",
        "    start_time = time.time()\n",
        "    y_pred_proba = model.predict_proba(X_test)\n",
        "    y_pred = model.predict(X_test)\n",
        "    end_time = time.time()\n",
        "\n",
        "    # Calculate detailed metrics\n",
        "    detailed_metrics = calculate_detailed_metrics(y_test, y_pred, \"XGBoost, Dataset A\")\n",
        "\n",
        "    # Calculate prediction time per sample in microseconds\n",
        "    prediction_time = (end_time - start_time) * 1000000 / len(X_test)\n",
        "    print(f\"Prediction time: {prediction_time:.2f} μs/sample\")\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    # Calculate macro and weighted F1 scores\n",
        "    macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
        "    weighted_f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "    # Get unique classes present in the test set\n",
        "    unique_test_classes = np.unique(y_test)\n",
        "\n",
        "    # Print class names if label encoder is provided\n",
        "    if label_encoder is not None:\n",
        "        # Filter class names to only include those present in the test set\n",
        "        class_names = [label_encoder.classes_[i] for i in unique_test_classes]\n",
        "    else:\n",
        "        class_names = None\n",
        "\n",
        "    # Print classification report with only the classes present in the test set\n",
        "    print(\"\\nClassification Report:\")\n",
        "    if class_names:\n",
        "        print(classification_report(y_test, y_pred,\n",
        "                                   labels=unique_test_classes,\n",
        "                                   target_names=class_names))\n",
        "    else:\n",
        "        print(classification_report(y_test, y_pred))\n",
        "\n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(y_test, y_pred, labels=unique_test_classes)\n",
        "\n",
        "    # Create a dictionary of metrics\n",
        "    metrics = {\n",
        "        'Accuracy': accuracy,\n",
        "        'Macro F1 Score': macro_f1,\n",
        "        'Weighted F1 Score': weighted_f1,\n",
        "        'Prediction Time (μs/sample)': prediction_time,\n",
        "        'False Positives': detailed_metrics['False Positives'],\n",
        "        'False Negatives': detailed_metrics['False Negatives'],\n",
        "        'Detection': detailed_metrics['Detection']\n",
        "    }\n",
        "\n",
        "    # Print metrics\n",
        "    print(\"\\nPerformance Metrics:\")\n",
        "    for metric, value in metrics.items():\n",
        "        print(f\"  {metric}: {value:.6f}\")\n",
        "\n",
        "    # Create confusion matrix plot\n",
        "    plt.figure(figsize=(16, 14))\n",
        "\n",
        "    # Use class names if available, otherwise use class indices\n",
        "    if class_names:\n",
        "        xticklabels = class_names\n",
        "        yticklabels = class_names\n",
        "    else:\n",
        "        xticklabels = unique_test_classes\n",
        "        yticklabels = unique_test_classes\n",
        "\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=xticklabels,\n",
        "                yticklabels=yticklabels)\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.xticks(rotation=90)\n",
        "    plt.yticks(rotation=0)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('confusion_matrix_multiclass.png')\n",
        "    plt.close()\n",
        "    print(\"Confusion matrix saved as 'confusion_matrix_multiclass.png'\")\n",
        "\n",
        "    # Create feature importance plot\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    xgb.plot_importance(model, max_num_features=20)\n",
        "    plt.title('Feature Importance')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('feature_importance_multiclass.png')\n",
        "    plt.close()\n",
        "    print(\"Feature importance plot saved as 'feature_importance_multiclass.png'\")\n",
        "\n",
        "    return metrics, cm\n",
        "\n",
        "def plot_multiclass_roc_curves(model, X_test, y_test, label_encoder=None):\n",
        "    \"\"\"\n",
        "    Plot ROC curves for each class in a one-vs-rest fashion\n",
        "\n",
        "    Args:\n",
        "        model: Trained XGBoost model\n",
        "        X_test: Test features\n",
        "        y_test: Test labels (encoded)\n",
        "        label_encoder: Label encoder to convert indices to original labels\n",
        "    \"\"\"\n",
        "    # Get predictions\n",
        "    y_score = model.predict_proba(X_test)\n",
        "\n",
        "    # Get number of classes\n",
        "    n_classes = y_score.shape[1]\n",
        "\n",
        "    # Get class names\n",
        "    class_names = label_encoder.classes_ if label_encoder is not None else [f\"Class {i}\" for i in range(n_classes)]\n",
        "\n",
        "    # Compute ROC curve and ROC area for each class\n",
        "    plt.figure(figsize=(12, 10))\n",
        "\n",
        "    # Compute macro-average ROC curve and ROC area\n",
        "    # First aggregate all false positive rates\n",
        "    all_fpr = np.unique(np.concatenate([np.linspace(0, 1, 100) for i in range(n_classes)]))\n",
        "\n",
        "    # Then interpolate all ROC curves at these points\n",
        "    mean_tpr = np.zeros_like(all_fpr)\n",
        "\n",
        "    # Plot ROC curves for each class\n",
        "    for i in range(n_classes):\n",
        "        # Convert to one-vs-rest\n",
        "        y_true_bin = (y_test == i).astype(int)\n",
        "        y_score_bin = y_score[:, i]\n",
        "\n",
        "        # Calculate ROC curve\n",
        "        fpr, tpr, _ = roc_curve(y_true_bin, y_score_bin)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "\n",
        "        # Plot class ROC curve\n",
        "        plt.plot(fpr, tpr, lw=2,\n",
        "                 label=f'{class_names[i]} (AUC = {roc_auc:.2f})')\n",
        "\n",
        "        # Interpolate tpr values for macro-average\n",
        "        mean_tpr += np.interp(all_fpr, fpr, tpr)\n",
        "\n",
        "    # Finish macro-average ROC curve\n",
        "    mean_tpr /= n_classes\n",
        "    mean_auc = auc(all_fpr, mean_tpr)\n",
        "    plt.plot(all_fpr, mean_tpr, 'k--',\n",
        "             label=f'Macro-average (AUC = {mean_auc:.2f})',\n",
        "             lw=3)\n",
        "\n",
        "    # Plot baseline\n",
        "    plt.plot([0, 1], [0, 1], 'r--', label='Random Classifier')\n",
        "\n",
        "    # Set plot details\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Multiclass ROC Curves (One-vs-Rest)')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save plot\n",
        "    plt.savefig('roc_curves_multiclass.png')\n",
        "    plt.close()\n",
        "    print(\"Multiclass ROC curves saved as 'roc_curves_multiclass.png'\")\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to run multiclass classification on the combined CICIDS2017 dataset\n",
        "    \"\"\"\n",
        "    from sklearn.model_selection import train_test_split\n",
        "\n",
        "    # First, make sure Google Drive is mounted\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        print(\"Mounting Google Drive...\")\n",
        "        drive.mount('/content/drive')\n",
        "        print(\"Google Drive mounted successfully.\")\n",
        "    except:\n",
        "        print(\"Not running in Colab or Drive already mounted.\")\n",
        "\n",
        "    # List files in the expected directory to check path\n",
        "    import os\n",
        "    cicids_dir = \"/content/drive/My Drive/CICIDS2017_improved\"\n",
        "    if os.path.exists(cicids_dir):\n",
        "        print(f\"\\nDirectory exists: {cicids_dir}\")\n",
        "        print(\"Files in directory:\")\n",
        "        for file in os.listdir(cicids_dir):\n",
        "            print(f\"  - {file}\")\n",
        "    else:\n",
        "        print(f\"\\nDirectory not found: {cicids_dir}\")\n",
        "        print(\"Searching for the dataset...\")\n",
        "\n",
        "        # Try to find the combined dataset by searching common locations\n",
        "        possible_paths = [\n",
        "            \"/content/drive/MyDrive/CICIDS2017_improved/combined_dataset.csv\",\n",
        "            \"/content/drive/My Drive/CICIDS2017/combined_dataset.csv\",\n",
        "            \"/content/drive/MyDrive/CICIDS2017/combined_dataset.csv\",\n",
        "            \"/content/combined_dataset.csv\"\n",
        "        ]\n",
        "\n",
        "        for path in possible_paths:\n",
        "            if os.path.exists(path):\n",
        "                print(f\"Found dataset at: {path}\")\n",
        "                combined_dataset_path = path\n",
        "                break\n",
        "        else:\n",
        "            # Let the user specify the path\n",
        "            print(\"\\nCannot find the combined dataset automatically.\")\n",
        "            combined_dataset_path = input(\"Please enter the full path to combined_dataset.csv: \")\n",
        "\n",
        "    # Define the file path to your combined dataset\n",
        "    combined_dataset_path = \"/content/drive/My Drive/CICIDS2017_improved/combined_dataset.csv\"\n",
        "\n",
        "    try:\n",
        "        print(f\"Loading combined dataset from {combined_dataset_path}...\")\n",
        "        combined_df = load_data(combined_dataset_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading combined dataset: {e}\")\n",
        "        # Ask for user input\n",
        "        combined_dataset_path = input(\"Please enter the correct path to the combined dataset CSV file: \")\n",
        "        try:\n",
        "            print(f\"Trying to load from {combined_dataset_path}...\")\n",
        "            combined_df = load_data(combined_dataset_path)\n",
        "        except Exception as e:\n",
        "            print(f\"Still unable to load the dataset: {e}\")\n",
        "            return\n",
        "\n",
        "    # Split the dataset into training and testing sets without stratification\n",
        "    print(\"\\nSplitting dataset into training (70%) and testing (30%) sets...\")\n",
        "\n",
        "    # Find the label column\n",
        "    label_col = None\n",
        "    for possible_label in ['Label', 'label', 'CLASS', 'class']:\n",
        "        if possible_label in combined_df.columns:\n",
        "            label_col = possible_label\n",
        "            break\n",
        "\n",
        "    if label_col is None:\n",
        "        print(\"Could not find label column in the combined dataset\")\n",
        "        return\n",
        "\n",
        "    # Print class distribution before split\n",
        "    print(\"\\nFull dataset class distribution:\")\n",
        "    class_counts = combined_df[label_col].value_counts()\n",
        "    for cls, count in class_counts.items():\n",
        "        print(f\"  {cls}: {count} ({count/len(combined_df)*100:.2f}%)\")\n",
        "\n",
        "    # Simple random split with no stratification\n",
        "    train_df, test_df = train_test_split(\n",
        "        combined_df,\n",
        "        test_size=0.3,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    print(f\"Training set: {len(train_df)} samples\")\n",
        "    print(f\"Test set: {len(test_df)} samples\")\n",
        "\n",
        "    # Display class distribution in splits\n",
        "    print(\"\\nTraining set class distribution:\")\n",
        "    train_class_dist = train_df[label_col].value_counts()\n",
        "    for cls, count in train_class_dist.items():\n",
        "        print(f\"  {cls}: {count} ({count/len(train_df)*100:.2f}%)\")\n",
        "\n",
        "    print(\"\\nTest set class distribution:\")\n",
        "    test_class_dist = test_df[label_col].value_counts()\n",
        "    for cls, count in test_class_dist.items():\n",
        "        print(f\"  {cls}: {count} ({count/len(test_df)*100:.2f}%)\")\n",
        "\n",
        "    # Preprocess the training data\n",
        "    try:\n",
        "        print(\"\\nPreprocessing training data...\")\n",
        "        train_processed, scaler, label_encoder = preprocess_data(\n",
        "            train_df,\n",
        "            fit_scaler=True,\n",
        "            fit_encoder=True\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"Error preprocessing training data: {e}\")\n",
        "        return\n",
        "\n",
        "    # Preprocess the test data using the same scaler and encoder\n",
        "    try:\n",
        "        print(\"\\nPreprocessing test data...\")\n",
        "        test_processed = preprocess_data(\n",
        "            test_df,\n",
        "            scaler=scaler,\n",
        "            label_encoder=label_encoder,\n",
        "            fit_scaler=False,\n",
        "            fit_encoder=False\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"Error preprocessing test data: {e}\")\n",
        "        return\n",
        "\n",
        "    # Split features and target for training data\n",
        "    X_train = train_processed.drop(['Label_Encoded', 'Label_Original'], axis=1)\n",
        "    y_train = train_processed['Label_Encoded'].astype(int)\n",
        "\n",
        "    # Split features and target for test data\n",
        "    X_test = test_processed.drop(['Label_Encoded', 'Label_Original'], axis=1)\n",
        "    y_test = test_processed['Label_Encoded'].astype(int)\n",
        "\n",
        "    print(f\"\\nTraining features shape: {X_train.shape}\")\n",
        "    print(f\"Training target shape: {y_train.shape}\")\n",
        "    print(f\"Test features shape: {X_test.shape}\")\n",
        "    print(f\"Test target shape: {y_test.shape}\")\n",
        "\n",
        "    # Handle class imbalance with SMOTE\n",
        "    X_train_balanced, y_train_balanced = handle_class_imbalance(X_train, y_train)\n",
        "\n",
        "    # Train XGBoost multiclass model\n",
        "    model = train_model(X_train_balanced, y_train_balanced)\n",
        "\n",
        "    # Evaluate model on test set\n",
        "    metrics, confusion_mat = evaluate_multiclass_model(model, X_test, y_test, label_encoder)\n",
        "\n",
        "    # Create a subset of 10k samples for additional evaluation\n",
        "    if len(X_test) > 10000:\n",
        "        print(\"\\nCreating 10k sample subset for additional evaluation...\")\n",
        "        idx = np.random.choice(len(X_test), 10000, replace=False)\n",
        "        X_subset = X_test.iloc[idx]\n",
        "        y_subset = y_test.iloc[idx]\n",
        "\n",
        "        print(\"\\nEvaluating on 10k subset:\")\n",
        "        print(f\"{'Model':<20}{'Accuracy':<12}{'F1 Score':<12}{'False Postive':<12}{'False Negative':<12}{'Detection':<12}\")\n",
        "        subset_metrics = calculate_detailed_metrics(y_subset, model.predict(X_subset), \"XGBoost, Dataset A, 10k subsample\")\n",
        "\n",
        "    # Plot ROC curves for multiclass\n",
        "    plot_multiclass_roc_curves(model, X_test, y_test, label_encoder)\n",
        "\n",
        "    # Save model for future use\n",
        "    model.save_model('xgboost_multiclass_model.json')\n",
        "    print(\"\\nModel saved as 'xgboost_multiclass_model.json'\")\n",
        "\n",
        "    # Print comparison with baseline\n",
        "    print(\"\\nPerformance Comparison:\")\n",
        "    print(f\"Baseline F1 Score (from paper): ~70%\")\n",
        "    print(f\"Our Model's Weighted F1 Score: {metrics['Weighted F1 Score']:.2%}\")\n",
        "    improvement = (metrics['Weighted F1 Score'] - 0.7) / 0.7 * 100\n",
        "    print(f\"Improvement: {improvement:.2f}%\")\n",
        "\n",
        "    print(\"\\nAnalysis complete!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import time\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
        "import xgboost as xgb\n",
        "import warnings\n",
        "import os\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def calculate_detailed_metrics(y_true, y_pred, model_name):\n",
        "    \"\"\"\n",
        "    Calculate detailed metrics for model evaluation.\n",
        "\n",
        "    Args:\n",
        "        y_true: True labels\n",
        "        y_pred: Predicted labels\n",
        "        model_name: Name of the model for display\n",
        "\n",
        "    Returns:\n",
        "        Dictionary of metrics\n",
        "    \"\"\"\n",
        "    # Basic metrics\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "\n",
        "    # Confusion matrix to get FP and FN\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    # For multi-class, calculate total false positives and false negatives\n",
        "    false_positives = 0\n",
        "    false_negatives = 0\n",
        "\n",
        "    # Loop through each class\n",
        "    for i in range(len(cm)):\n",
        "        # False positives are sum of column i minus value at position (i,i)\n",
        "        false_positives += sum(cm[:,i]) - cm[i,i]\n",
        "        # False negatives are sum of row i minus value at position (i,i)\n",
        "        false_negatives += sum(cm[i,:]) - cm[i,i]\n",
        "\n",
        "    # Calculate detection rate (True Positives / (True Positives + False Negatives))\n",
        "    # For multiclass, detection is macro-averaged recall\n",
        "    detection = np.sum(np.diag(cm)) / np.sum(cm)\n",
        "\n",
        "    # Print results in tabular format\n",
        "    print(f\"{model_name:<20}{accuracy:.8f}{f1:.8f}{false_positives:<12}{false_negatives:<12}{detection:.8f}\")\n",
        "\n",
        "    # Return metrics dict\n",
        "    return {\n",
        "        'Accuracy': accuracy,\n",
        "        'F1 Score': f1,\n",
        "        'False Positives': false_positives,\n",
        "        'False Negatives': false_negatives,\n",
        "        'Detection': detection\n",
        "    }\n",
        "\n",
        "def load_data_by_days(base_dir):\n",
        "    \"\"\"\n",
        "    Load individual day-wise datasets and combine them for training and testing.\n",
        "    \"\"\"\n",
        "    train_days = [\"monday.csv\", \"tuesday.csv\", \"wednesday.csv\"]\n",
        "    test_days = [\"thursday.csv\", \"friday.csv\"]\n",
        "\n",
        "    train_df_list, test_df_list = [], []\n",
        "\n",
        "    for file in train_days:\n",
        "        path = os.path.join(base_dir, file)\n",
        "        if os.path.exists(path):\n",
        "            df = pd.read_csv(path, delimiter=',', low_memory=False)\n",
        "            train_df_list.append(df)\n",
        "        else:\n",
        "            print(f\"Warning: {file} not found\")\n",
        "\n",
        "    for file in test_days:\n",
        "        path = os.path.join(base_dir, file)\n",
        "        if os.path.exists(path):\n",
        "            df = pd.read_csv(path, delimiter=',', low_memory=False)\n",
        "            test_df_list.append(df)\n",
        "        else:\n",
        "            print(f\"Warning: {file} not found\")\n",
        "\n",
        "    train_df = pd.concat(train_df_list, ignore_index=True)\n",
        "    test_df = pd.concat(test_df_list, ignore_index=True)\n",
        "\n",
        "    print(f\"Training dataset shape: {train_df.shape}\")\n",
        "    print(f\"Testing dataset shape: {test_df.shape}\")\n",
        "\n",
        "    return train_df, test_df\n",
        "\n",
        "def preprocess_data(df, scaler=None, label_encoder=None, fit_scaler=False, fit_encoder=False):\n",
        "    \"\"\"\n",
        "    Preprocess the dataset by cleaning, encoding labels, and normalizing features.\n",
        "    Handles unseen labels in the test set by mapping them to 'UNKNOWN_ATTACK'.\n",
        "    \"\"\"\n",
        "    print(\"\\nPreprocessing data:\")\n",
        "    print(f\"Initial columns: {df.columns.tolist()[:5]}... (total: {len(df.columns)})\")\n",
        "\n",
        "    df_processed = df.copy()\n",
        "    label_col = None\n",
        "    for possible_label in ['Label', 'label', 'CLASS', 'class']:\n",
        "        if possible_label in df_processed.columns:\n",
        "            label_col = possible_label\n",
        "            break\n",
        "\n",
        "    if label_col is None:\n",
        "        raise ValueError(\"Label column not found\")\n",
        "\n",
        "    original_label = df_processed[label_col].copy()\n",
        "\n",
        "    # Check label distribution\n",
        "    label_counts = df_processed[label_col].value_counts()\n",
        "    print(f\"Label distribution: {label_counts.to_dict()}\")\n",
        "\n",
        "    if fit_encoder:\n",
        "        label_encoder = LabelEncoder()\n",
        "        df_processed['Label_Encoded'] = label_encoder.fit_transform(original_label)\n",
        "\n",
        "        # Display mapping\n",
        "        print(\"\\nLabel encoding mapping:\")\n",
        "        for i, label in enumerate(label_encoder.classes_):\n",
        "            print(f\"  {i}: {label}\")\n",
        "    else:\n",
        "        unknown_labels = set(original_label) - set(label_encoder.classes_)\n",
        "        if unknown_labels:\n",
        "            print(f\"⚠️ Warning: Found unseen labels in test data: {unknown_labels}\")\n",
        "            df_processed[label_col] = df_processed[label_col].apply(\n",
        "                lambda x: x if x in label_encoder.classes_ else 'UNKNOWN_ATTACK'\n",
        "            )\n",
        "\n",
        "        if 'UNKNOWN_ATTACK' not in label_encoder.classes_:\n",
        "            all_classes = list(label_encoder.classes_) + ['UNKNOWN_ATTACK']\n",
        "            label_encoder.classes_ = np.array(all_classes)\n",
        "\n",
        "        df_processed['Label_Encoded'] = label_encoder.transform(df_processed[label_col])\n",
        "\n",
        "    drop_cols = ['Flow ID', 'Src IP', 'Src Port', 'Dst IP', 'Dst Port', 'Timestamp', label_col, 'id']\n",
        "    columns_to_drop = [col for col in drop_cols if col in df_processed.columns]\n",
        "    df_cleaned = df_processed.drop(columns=columns_to_drop, errors='ignore')\n",
        "\n",
        "    # Convert all columns to numeric, coercing errors to NaN\n",
        "    numeric_cols = []\n",
        "    for col in df_cleaned.columns:\n",
        "        if col != 'Label_Encoded' and col != 'Label_Original':\n",
        "            try:\n",
        "                df_cleaned[col] = pd.to_numeric(df_cleaned[col], errors='coerce')\n",
        "                numeric_cols.append(col)\n",
        "            except Exception as e:\n",
        "                print(f\"Error converting column {col} to numeric: {e}\")\n",
        "                df_cleaned = df_cleaned.drop(columns=[col])\n",
        "\n",
        "    # Display info about numeric columns\n",
        "    print(f\"Number of numeric columns after conversion: {len(numeric_cols)}\")\n",
        "    if len(numeric_cols) == 0:\n",
        "        raise ValueError(\"No numeric columns available after preprocessing\")\n",
        "\n",
        "    # Handle NaN values\n",
        "    print(f\"NaN values before handling: {df_cleaned[numeric_cols].isna().sum().sum()}\")\n",
        "\n",
        "    df_cleaned.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "    # Handle outliers by clipping\n",
        "    for col in numeric_cols:\n",
        "        upper_limit = df_cleaned[col].quantile(0.99)\n",
        "        lower_limit = df_cleaned[col].quantile(0.01)\n",
        "        df_cleaned[col] = np.clip(df_cleaned[col], lower_limit, upper_limit)\n",
        "\n",
        "    # Fill remaining NaN values with column means\n",
        "    for col in numeric_cols:\n",
        "        if df_cleaned[col].isna().any():\n",
        "            col_mean = df_cleaned[col].mean()\n",
        "            df_cleaned[col] = df_cleaned[col].fillna(col_mean)\n",
        "\n",
        "    print(f\"NaN values after handling: {df_cleaned[numeric_cols].isna().sum().sum()}\")\n",
        "\n",
        "    features = df_cleaned[numeric_cols]\n",
        "    labels = df_cleaned['Label_Encoded']\n",
        "\n",
        "    if fit_scaler:\n",
        "        scaler = MinMaxScaler()\n",
        "        scaled_features = scaler.fit_transform(features)\n",
        "    else:\n",
        "        scaled_features = scaler.transform(features)\n",
        "\n",
        "    scaled_df = pd.DataFrame(scaled_features, columns=features.columns)\n",
        "    scaled_df['Label_Encoded'] = labels.values\n",
        "    scaled_df['Label_Original'] = original_label.values\n",
        "\n",
        "    print(f\"Preprocessed dataset shape: {scaled_df.shape}\")\n",
        "    print(f\"Number of unique classes: {scaled_df['Label_Encoded'].nunique()}\")\n",
        "\n",
        "    return scaled_df, scaler, label_encoder\n",
        "\n",
        "def train_model(X_train, y_train, X_val=None, y_val=None, params=None):\n",
        "    \"\"\"\n",
        "    Train an XGBoost model for multiclass classification\n",
        "\n",
        "    Args:\n",
        "        X_train: Training features\n",
        "        y_train: Training labels\n",
        "        X_val: Validation features\n",
        "        y_val: Validation labels\n",
        "        params: XGBoost parameters\n",
        "\n",
        "    Returns:\n",
        "        Trained XGBoost model\n",
        "    \"\"\"\n",
        "    print(\"\\nTraining XGBoost model for multiclass classification...\")\n",
        "\n",
        "    # Count number of classes\n",
        "    num_classes = len(np.unique(y_train))\n",
        "    print(f\"Number of classes: {num_classes}\")\n",
        "\n",
        "    # Set default parameters if not provided\n",
        "    if params is None:\n",
        "        params = {\n",
        "            'objective': 'multi:softprob',\n",
        "            'num_class': num_classes,\n",
        "            'eta': 0.1,\n",
        "            'max_depth': 6,\n",
        "            'min_child_weight': 1,\n",
        "            'subsample': 0.8,\n",
        "            'colsample_bytree': 0.8,\n",
        "            'tree_method': 'hist',  # For faster training\n",
        "            'eval_metric': 'mlogloss',\n",
        "            'use_label_encoder': False\n",
        "        }\n",
        "\n",
        "    # Create XGBoost model\n",
        "    model = xgb.XGBClassifier(**params)\n",
        "\n",
        "    # Train with validation if provided\n",
        "    if X_val is not None and y_val is not None:\n",
        "        print(\"Training with validation set...\")\n",
        "        eval_set = [(X_train, y_train), (X_val, y_val)]\n",
        "        model.fit(\n",
        "            X_train, y_train,\n",
        "            eval_set=eval_set,\n",
        "            early_stopping_rounds=10,\n",
        "            verbose=True\n",
        "        )\n",
        "    else:\n",
        "        print(\"Training without validation set...\")\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "    return model\n",
        "\n",
        "def evaluate_model(model, X_test, y_test, label_encoder):\n",
        "    \"\"\"Evaluate the trained model on the test dataset.\"\"\"\n",
        "    print(\"\\nEvaluating multiclass model...\")\n",
        "\n",
        "    # Print metrics header\n",
        "    print(f\"\\n{'Model':<20}{'Accuracy':<12}{'F1 Score':<12}{'False Postive':<12}{'False Negative':<12}{'Detection':<12}\")\n",
        "\n",
        "    # Predict and time\n",
        "    start_time = time.time()\n",
        "    y_pred = model.predict(X_test)\n",
        "    end_time = time.time()\n",
        "\n",
        "    # Calculate prediction time per sample in microseconds\n",
        "    prediction_time = (end_time - start_time) * 1000000 / len(X_test)\n",
        "\n",
        "    # Calculate detailed metrics\n",
        "    detailed_metrics = calculate_detailed_metrics(y_test, y_pred, \"XGBoost, Dataset B\")\n",
        "\n",
        "    # Calculate standard metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
        "    weighted_f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "    print(f\"\\nPrediction time: {prediction_time:.2f} μs/sample\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Macro F1 Score: {macro_f1:.4f}\")\n",
        "    print(f\"Weighted F1 Score: {weighted_f1:.4f}\")\n",
        "\n",
        "    unique_test_classes = np.unique(y_test)\n",
        "    filtered_class_names = [label_encoder.classes_[i] for i in unique_test_classes]\n",
        "\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test, y_pred, labels=unique_test_classes, target_names=filtered_class_names))\n",
        "\n",
        "    # Create confusion matrix plot\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    plt.figure(figsize=(16, 14))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=filtered_class_names,\n",
        "                yticklabels=filtered_class_names)\n",
        "    plt.title('Confusion Matrix - Dataset B (Temporal Split)')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.xticks(rotation=90)\n",
        "    plt.yticks(rotation=0)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('confusion_matrix_dataset_b.png')\n",
        "    plt.close()\n",
        "    print(\"Confusion matrix saved as 'confusion_matrix_dataset_b.png'\")\n",
        "\n",
        "    # Create feature importance plot\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    xgb.plot_importance(model, max_num_features=20)\n",
        "    plt.title('Feature Importance - Dataset B')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('feature_importance_dataset_b.png')\n",
        "    plt.close()\n",
        "    print(\"Feature importance plot saved as 'feature_importance_dataset_b.png'\")\n",
        "\n",
        "    return detailed_metrics\n",
        "\n",
        "def main():\n",
        "    # First, make sure Google Drive is mounted\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        print(\"Mounting Google Drive...\")\n",
        "        drive.mount('/content/drive')\n",
        "        print(\"Google Drive mounted successfully.\")\n",
        "    except:\n",
        "        print(\"Not running in Colab or Drive already mounted.\")\n",
        "\n",
        "    base_dir = \"/content/drive/My Drive/CICIDS2017_improved\"\n",
        "\n",
        "    # Check if directory exists\n",
        "    if os.path.exists(base_dir):\n",
        "        print(f\"\\nDirectory exists: {base_dir}\")\n",
        "        print(\"Files in directory:\")\n",
        "        for file in os.listdir(base_dir):\n",
        "            print(f\"  - {file}\")\n",
        "    else:\n",
        "        print(f\"\\nDirectory not found: {base_dir}\")\n",
        "        return\n",
        "\n",
        "    # Load data by days (temporal split)\n",
        "    train_df, test_df = load_data_by_days(base_dir)\n",
        "\n",
        "    # Preprocess training data\n",
        "    train_processed, scaler, label_encoder = preprocess_data(train_df, fit_scaler=True, fit_encoder=True)\n",
        "\n",
        "    # Preprocess test data\n",
        "    test_processed, _, _ = preprocess_data(test_df, scaler=scaler, label_encoder=label_encoder)\n",
        "\n",
        "    # Split features and target for training data\n",
        "    X_train = train_processed.drop(columns=['Label_Encoded', 'Label_Original'])\n",
        "    y_train = train_processed['Label_Encoded'].astype(int)\n",
        "\n",
        "    # Split features and target for test data\n",
        "    X_test = test_processed.drop(columns=['Label_Encoded', 'Label_Original'])\n",
        "    y_test = test_processed['Label_Encoded'].astype(int)\n",
        "\n",
        "    print(f\"\\nTraining features shape: {X_train.shape}\")\n",
        "    print(f\"Training target shape: {y_train.shape}\")\n",
        "    print(f\"Test features shape: {X_test.shape}\")\n",
        "    print(f\"Test target shape: {y_test.shape}\")\n",
        "\n",
        "    # Train XGBoost model\n",
        "    model = train_model(X_train, y_train)\n",
        "\n",
        "    # Evaluate model\n",
        "    metrics = evaluate_model(model, X_test, y_test, label_encoder)\n",
        "\n",
        "    # Create a subset of 10k samples for additional evaluation\n",
        "    if len(X_test) > 10000:\n",
        "        print(\"\\nCreating 10k sample subset for additional evaluation...\")\n",
        "        idx = np.random.choice(len(X_test), 10000, replace=False)\n",
        "        X_subset = X_test.iloc[idx]\n",
        "        y_subset = y_test.iloc[idx]\n",
        "\n",
        "        print(\"\\nEvaluating on 10k subset:\")\n",
        "        print(f\"{'Model':<20}{'Accuracy':<12}{'F1 Score':<12}{'False Postive':<12}{'False Negative':<12}{'Detection':<12}\")\n",
        "        subset_metrics = calculate_detailed_metrics(y_subset, model.predict(X_subset), \"XGBoost, Dataset B, 10k subsample\")\n",
        "\n",
        "    # Save model\n",
        "    model.save_model('xgboost_dataset_b_model.json')\n",
        "    print(\"\\nModel saved as 'xgboost_dataset_b_model.json'\")\n",
        "\n",
        "    print(\"\\nAnalysis complete!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "0FVGDz1QOuP0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ecf56cbd-567f-4663-afcc-731a86d9f02a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounting Google Drive...\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Google Drive mounted successfully.\n",
            "\n",
            "Directory exists: /content/drive/My Drive/CICIDS2017_improved\n",
            "Files in directory:\n",
            "  - monday.csv\n",
            "  - tuesday.csv\n",
            "  - wednesday.csv\n",
            "  - thursday.csv\n",
            "  - friday.csv\n",
            "  - combined_dataset.csv\n",
            "  - balanced_multiclass_10k.csv\n",
            "  - train.csv\n",
            "  - metrics_by_class.png\n",
            "  - class_metrics.csv\n",
            "  - prediction_results.csv\n",
            "  - confusion_matrix.csv\n",
            "  - confusion_metrics_by_class.png\n",
            "  - fixed_train.csv\n",
            "  - fixed_test.csv\n",
            "  - enhanced_train.csv\n",
            "  - plots_multiclass\n",
            "  - multiclass_metrics.csv\n",
            "Training dataset shape: (1190343, 91)\n",
            "Testing dataset shape: (909633, 91)\n",
            "\n",
            "Preprocessing data:\n",
            "Initial columns: ['id', 'Flow ID', 'Src IP', 'Src Port', 'Dst IP']... (total: 91)\n",
            "Label distribution: {'BENIGN': 1005850, 'DoS Hulk': 158468, 'DoS GoldenEye': 7567, 'FTP-Patator': 3972, 'DoS Slowloris': 3859, 'DoS Slowhttptest - Attempted': 3368, 'SSH-Patator': 2961, 'DoS Slowloris - Attempted': 1847, 'DoS Slowhttptest': 1740, 'DoS Hulk - Attempted': 581, 'DoS GoldenEye - Attempted': 80, 'SSH-Patator - Attempted': 27, 'FTP-Patator - Attempted': 12, 'Heartbleed': 11}\n",
            "\n",
            "Label encoding mapping:\n",
            "  0: BENIGN\n",
            "  1: DoS GoldenEye\n",
            "  2: DoS GoldenEye - Attempted\n",
            "  3: DoS Hulk\n",
            "  4: DoS Hulk - Attempted\n",
            "  5: DoS Slowhttptest\n",
            "  6: DoS Slowhttptest - Attempted\n",
            "  7: DoS Slowloris\n",
            "  8: DoS Slowloris - Attempted\n",
            "  9: FTP-Patator\n",
            "  10: FTP-Patator - Attempted\n",
            "  11: Heartbleed\n",
            "  12: SSH-Patator\n",
            "  13: SSH-Patator - Attempted\n",
            "Number of numeric columns after conversion: 83\n",
            "NaN values before handling: 0\n",
            "NaN values after handling: 0\n",
            "Preprocessed dataset shape: (1190343, 85)\n",
            "Number of unique classes: 14\n",
            "\n",
            "Preprocessing data:\n",
            "Initial columns: ['id', 'Flow ID', 'Src IP', 'Src Port', 'Dst IP']... (total: 91)\n",
            "Label distribution: {'BENIGN': 576716, 'Portscan': 159066, 'DDoS': 95144, 'Infiltration - Portscan': 71767, 'Botnet - Attempted': 4067, 'Web Attack - Brute Force - Attempted': 1292, 'Botnet': 736, 'Web Attack - XSS - Attempted': 655, 'Web Attack - Brute Force': 73, 'Infiltration - Attempted': 45, 'Infiltration': 36, 'Web Attack - XSS': 18, 'Web Attack - SQL Injection': 13, 'Web Attack - SQL Injection - Attempted': 5}\n",
            "⚠️ Warning: Found unseen labels in test data: {'Infiltration - Portscan', 'Web Attack - XSS', 'Web Attack - Brute Force', 'Infiltration - Attempted', 'Infiltration', 'Web Attack - SQL Injection - Attempted', 'Portscan', 'Botnet - Attempted', 'Botnet', 'Web Attack - SQL Injection', 'Web Attack - Brute Force - Attempted', 'Web Attack - XSS - Attempted', 'DDoS'}\n",
            "Number of numeric columns after conversion: 83\n",
            "NaN values before handling: 0\n",
            "NaN values after handling: 0\n",
            "Preprocessed dataset shape: (909633, 85)\n",
            "Number of unique classes: 2\n",
            "\n",
            "Training features shape: (1190343, 83)\n",
            "Training target shape: (1190343,)\n",
            "Test features shape: (909633, 83)\n",
            "Test target shape: (909633,)\n",
            "\n",
            "Training XGBoost model for multiclass classification...\n",
            "Number of classes: 14\n",
            "Training without validation set...\n",
            "\n",
            "Evaluating multiclass model...\n",
            "\n",
            "Model               Accuracy    F1 Score    False PostiveFalse NegativeDetection   \n",
            "XGBoost, Dataset B  0.634004040.52400960332922      332922      0.63400404\n",
            "\n",
            "Prediction time: 17.39 μs/sample\n",
            "Accuracy: 0.6340\n",
            "Macro F1 Score: 0.1378\n",
            "Weighted F1 Score: 0.5240\n",
            "\n",
            "Classification Report:\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "        BENIGN       0.70      1.00      0.83    576716\n",
            "UNKNOWN_ATTACK       0.00      0.00      0.00    332917\n",
            "\n",
            "     micro avg       0.70      0.63      0.67    909633\n",
            "     macro avg       0.35      0.50      0.41    909633\n",
            "  weighted avg       0.45      0.63      0.52    909633\n",
            "\n",
            "Confusion matrix saved as 'confusion_matrix_dataset_b.png'\n",
            "Feature importance plot saved as 'feature_importance_dataset_b.png'\n",
            "\n",
            "Creating 10k sample subset for additional evaluation...\n",
            "\n",
            "Evaluating on 10k subset:\n",
            "Model               Accuracy    F1 Score    False PostiveFalse NegativeDetection   \n",
            "XGBoost, Dataset B, 10k subsample0.627700000.516189303723        3723        0.62770000\n",
            "\n",
            "Model saved as 'xgboost_dataset_b_model.json'\n",
            "\n",
            "Analysis complete!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x1000 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import time\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "import xgboost as xgb\n",
        "import warnings\n",
        "import os\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def calculate_detailed_metrics(y_true, y_pred, model_name):\n",
        "    \"\"\"\n",
        "    Calculate detailed metrics for model evaluation.\n",
        "\n",
        "    Args:\n",
        "        y_true: True labels\n",
        "        y_pred: Predicted labels\n",
        "        model_name: Name of the model for display\n",
        "\n",
        "    Returns:\n",
        "        Dictionary of metrics\n",
        "    \"\"\"\n",
        "    # Basic metrics\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "\n",
        "    # Confusion matrix to get FP and FN\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    # For multi-class, calculate total false positives and false negatives\n",
        "    false_positives = 0\n",
        "    false_negatives = 0\n",
        "\n",
        "    # Loop through each class\n",
        "    for i in range(len(cm)):\n",
        "        # False positives are sum of column i minus value at position (i,i)\n",
        "        false_positives += sum(cm[:,i]) - cm[i,i]\n",
        "        # False negatives are sum of row i minus value at position (i,i)\n",
        "        false_negatives += sum(cm[i,:]) - cm[i,i]\n",
        "\n",
        "    # Calculate detection rate (True Positives / (True Positives + False Negatives))\n",
        "    # For multiclass, detection is macro-averaged recall\n",
        "    detection = np.sum(np.diag(cm)) / np.sum(cm)\n",
        "\n",
        "    # Print results in tabular format\n",
        "    print(f\"{model_name:<20}{accuracy:.8f}{f1:.8f}{false_positives:<12}{false_negatives:<12}{detection:.8f}\")\n",
        "\n",
        "    # Return metrics dict\n",
        "    return {\n",
        "        'Accuracy': accuracy,\n",
        "        'F1 Score': f1,\n",
        "        'False Positives': false_positives,\n",
        "        'False Negatives': false_negatives,\n",
        "        'Detection': detection\n",
        "    }\n",
        "\n",
        "def preprocess_data(df, scaler=None, label_encoder=None, fit_scaler=False, fit_encoder=False):\n",
        "    \"\"\"\n",
        "    Preprocess the dataset by cleaning, encoding labels, and normalizing features.\n",
        "    \"\"\"\n",
        "    print(\"\\nPreprocessing data:\")\n",
        "    print(f\"Initial shape: {df.shape}\")\n",
        "\n",
        "    # Make a copy to avoid modifying the original\n",
        "    df_processed = df.copy()\n",
        "\n",
        "    # Identify the label column\n",
        "    label_col = 'Label'  # From the CSV info, we know it's called 'Label'\n",
        "    if label_col not in df_processed.columns:\n",
        "        raise ValueError(f\"Label column '{label_col}' not found in the dataset\")\n",
        "\n",
        "    # Check label distribution\n",
        "    label_counts = df_processed[label_col].value_counts()\n",
        "    print(f\"Label distribution: {label_counts.to_dict()}\")\n",
        "\n",
        "    # Keep the original label\n",
        "    original_label = df_processed[label_col].copy()\n",
        "\n",
        "    # Label encode the attack types\n",
        "    if fit_encoder:\n",
        "        label_encoder = LabelEncoder()\n",
        "        encoded_labels = label_encoder.fit_transform(original_label)\n",
        "\n",
        "        # Display mapping\n",
        "        print(\"\\nLabel encoding mapping:\")\n",
        "        for i, label in enumerate(label_encoder.classes_):\n",
        "            print(f\"  {i}: {label}\")\n",
        "    else:\n",
        "        encoded_labels = label_encoder.transform(original_label)\n",
        "\n",
        "    # Add encoded labels column\n",
        "    df_processed['Label_Encoded'] = encoded_labels\n",
        "\n",
        "    # Remove identifiers and other non-feature columns\n",
        "    columns_to_drop = [\n",
        "        'id', 'Flow ID', 'Src IP', 'Src Port', 'Dst IP', 'Dst Port',\n",
        "        'Protocol', 'Timestamp', 'Label'\n",
        "    ]\n",
        "\n",
        "    # Only drop columns that exist in the dataframe\n",
        "    columns_to_drop = [col for col in columns_to_drop if col in df_processed.columns]\n",
        "    df_cleaned = df_processed.drop(columns=columns_to_drop, errors='ignore')\n",
        "\n",
        "    # Convert all columns to numeric except Label_Encoded\n",
        "    numeric_cols = []\n",
        "    for col in df_cleaned.columns:\n",
        "        if col != 'Label_Encoded':\n",
        "            try:\n",
        "                df_cleaned[col] = pd.to_numeric(df_cleaned[col], errors='coerce')\n",
        "                numeric_cols.append(col)\n",
        "            except Exception as e:\n",
        "                print(f\"Error converting column {col} to numeric: {e}\")\n",
        "                df_cleaned = df_cleaned.drop(columns=[col])\n",
        "\n",
        "    # Display info about numeric columns\n",
        "    print(f\"Number of numeric columns after conversion: {len(numeric_cols)}\")\n",
        "\n",
        "    # Handle NaN values\n",
        "    print(f\"NaN values before handling: {df_cleaned[numeric_cols].isna().sum().sum()}\")\n",
        "\n",
        "    # Replace infinity values with NaN\n",
        "    df_cleaned = df_cleaned.replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "    # Handle outliers with quantile clipping\n",
        "    for col in numeric_cols:\n",
        "        upper_limit = df_cleaned[col].quantile(0.99)\n",
        "        lower_limit = df_cleaned[col].quantile(0.01)\n",
        "        df_cleaned[col] = np.clip(df_cleaned[col], lower_limit, upper_limit)\n",
        "\n",
        "    # Fill NaN values with column means\n",
        "    for col in numeric_cols:\n",
        "        if df_cleaned[col].isna().any():\n",
        "            col_mean = df_cleaned[col].mean()\n",
        "            df_cleaned[col] = df_cleaned[col].fillna(col_mean)\n",
        "\n",
        "    print(f\"NaN values after handling: {df_cleaned[numeric_cols].isna().sum().sum()}\")\n",
        "\n",
        "    # Apply min-max scaling to numeric columns\n",
        "    features = df_cleaned[numeric_cols]\n",
        "    labels = df_cleaned['Label_Encoded']\n",
        "\n",
        "    # Apply scaling\n",
        "    if fit_scaler:\n",
        "        scaler = MinMaxScaler()\n",
        "        scaled_features = scaler.fit_transform(features)\n",
        "    else:\n",
        "        scaled_features = scaler.transform(features)\n",
        "\n",
        "    # Create a new dataframe with scaled features\n",
        "    scaled_df = pd.DataFrame(scaled_features, columns=features.columns)\n",
        "\n",
        "    # Add back the encoded label column\n",
        "    scaled_df['Label_Encoded'] = labels.values\n",
        "\n",
        "    print(f\"Preprocessed dataset shape: {scaled_df.shape}\")\n",
        "    print(f\"Number of unique classes: {scaled_df['Label_Encoded'].nunique()}\")\n",
        "\n",
        "    # Return processed dataframe and optionally scaler and encoder\n",
        "    if fit_scaler or fit_encoder:\n",
        "        return scaled_df, scaler, label_encoder\n",
        "    else:\n",
        "        return scaled_df\n",
        "\n",
        "def train_and_evaluate_model(X_train, y_train, X_test, y_test, label_encoder):\n",
        "    \"\"\"\n",
        "    Train an XGBoost model and evaluate its performance\n",
        "    \"\"\"\n",
        "    print(\"\\nTraining XGBoost model...\")\n",
        "\n",
        "    # Count number of classes\n",
        "    num_classes = len(np.unique(y_train))\n",
        "    print(f\"Number of classes: {num_classes}\")\n",
        "\n",
        "    # Set parameters\n",
        "    params = {\n",
        "        'objective': 'multi:softprob',\n",
        "        'num_class': num_classes,\n",
        "        'eta': 0.1,\n",
        "        'max_depth': 6,\n",
        "        'min_child_weight': 1,\n",
        "        'subsample': 0.8,\n",
        "        'colsample_bytree': 0.8,\n",
        "        'tree_method': 'hist',\n",
        "        'eval_metric': 'mlogloss',\n",
        "        'use_label_encoder': False\n",
        "    }\n",
        "\n",
        "    # Create and train the model\n",
        "    model = xgb.XGBClassifier(**params)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate the model\n",
        "    print(\"\\nEvaluating model...\")\n",
        "\n",
        "    # Print metrics header\n",
        "    print(f\"\\n{'Model':<20}{'Accuracy':<12}{'F1 Score':<12}{'False Postive':<12}{'False Negative':<12}{'Detection':<12}\")\n",
        "\n",
        "    # Predict and time\n",
        "    start_time = time.time()\n",
        "    y_pred = model.predict(X_test)\n",
        "    end_time = time.time()\n",
        "\n",
        "    # Calculate prediction time per sample\n",
        "    prediction_time = (end_time - start_time) * 1000000 / len(X_test)\n",
        "    print(f\"Prediction time: {prediction_time:.2f} μs/sample\")\n",
        "\n",
        "    # Calculate detailed metrics\n",
        "    detailed_metrics = calculate_detailed_metrics(y_test, y_pred, \"XGBoost, Dataset B\")\n",
        "\n",
        "    # Generate classification report\n",
        "    unique_test_classes = np.unique(y_test)\n",
        "    class_names = [label_encoder.classes_[i] for i in unique_test_classes]\n",
        "\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test, y_pred, labels=unique_test_classes, target_names=class_names))\n",
        "\n",
        "    # Create confusion matrix visualization\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=class_names,\n",
        "                yticklabels=class_names)\n",
        "    plt.title('Confusion Matrix - Dataset B')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.xticks(rotation=90)\n",
        "    plt.yticks(rotation=0)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('confusion_matrix_balanced_dataset_b.png')\n",
        "    plt.close()\n",
        "    print(\"Confusion matrix saved as 'confusion_matrix_balanced_dataset_b.png'\")\n",
        "\n",
        "    # Create feature importance plot\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    xgb.plot_importance(model, max_num_features=20)\n",
        "    plt.title('Feature Importance - Dataset B')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('feature_importance_balanced_dataset_b.png')\n",
        "    plt.close()\n",
        "    print(\"Feature importance plot saved as 'feature_importance_balanced_dataset_b.png'\")\n",
        "\n",
        "    return model, detailed_metrics\n",
        "\n",
        "def main():\n",
        "    # Mount Google Drive if running in Colab\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        print(\"Mounting Google Drive...\")\n",
        "        drive.mount('/content/drive')\n",
        "        print(\"Google Drive mounted successfully.\")\n",
        "    except:\n",
        "        print(\"Not running in Colab or Drive already mounted.\")\n",
        "\n",
        "    # Set base directory\n",
        "    base_dir = \"/content/drive/My Drive/CICIDS2017_improved\"\n",
        "\n",
        "    # Search for the file in the same directory\n",
        "    print(f\"\\nSearching for balanced_10k_test.csv in {base_dir}...\")\n",
        "\n",
        "    # List all files in the directory to help find the file\n",
        "    if os.path.exists(base_dir):\n",
        "        print(\"Files in directory:\")\n",
        "        for file in os.listdir(base_dir):\n",
        "            print(f\"  - {file}\")\n",
        "\n",
        "        # Look for files containing \"10k\" in the name\n",
        "        balanced_files = [f for f in os.listdir(base_dir) if \"10k\" in f.lower()]\n",
        "        if balanced_files:\n",
        "            print(\"\\nPossible balanced dataset files found:\")\n",
        "            for file in balanced_files:\n",
        "                print(f\"  - {file}\")\n",
        "\n",
        "            # Use the first matching file\n",
        "            if balanced_files:\n",
        "                balanced_filename = balanced_files[0]\n",
        "                balanced_test_path = os.path.join(base_dir, balanced_filename)\n",
        "                print(f\"\\nUsing file: {balanced_filename}\")\n",
        "        else:\n",
        "            # If no files with \"10k\" found, prompt for path\n",
        "            print(\"\\nNo files with '10k' in the name found.\")\n",
        "            balanced_test_path = input(\"Please enter the full path to balanced_10k_test.csv: \")\n",
        "    else:\n",
        "        print(f\"Directory not found: {base_dir}\")\n",
        "        balanced_test_path = input(\"Please enter the full path to balanced_10k_test.csv: \")\n",
        "\n",
        "    # Load the dataset\n",
        "    print(f\"Loading balanced 10k test dataset from {balanced_test_path}...\")\n",
        "    try:\n",
        "        df = pd.read_csv(balanced_test_path)\n",
        "        print(f\"Dataset loaded successfully. Shape: {df.shape}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading dataset: {e}\")\n",
        "        return\n",
        "\n",
        "    # Split data into training and testing sets\n",
        "    print(\"\\nSplitting dataset into 70% training and 30% testing...\")\n",
        "    train_df, test_df = train_test_split(df, test_size=0.3, random_state=42)\n",
        "    print(f\"Training set: {len(train_df)} samples\")\n",
        "    print(f\"Test set: {len(test_df)} samples\")\n",
        "\n",
        "    # Preprocess training data\n",
        "    train_processed, scaler, label_encoder = preprocess_data(\n",
        "        train_df, fit_scaler=True, fit_encoder=True\n",
        "    )\n",
        "\n",
        "    # Preprocess test data\n",
        "    test_processed = preprocess_data(\n",
        "        test_df, scaler=scaler, label_encoder=label_encoder,\n",
        "        fit_scaler=False, fit_encoder=False\n",
        "    )\n",
        "\n",
        "    # Prepare features and labels\n",
        "    X_train = train_processed.drop('Label_Encoded', axis=1)\n",
        "    y_train = train_processed['Label_Encoded']\n",
        "    X_test = test_processed.drop('Label_Encoded', axis=1)\n",
        "    y_test = test_processed['Label_Encoded']\n",
        "\n",
        "    print(f\"\\nTraining features shape: {X_train.shape}\")\n",
        "    print(f\"Training target shape: {y_train.shape}\")\n",
        "    print(f\"Test features shape: {X_test.shape}\")\n",
        "    print(f\"Test target shape: {y_test.shape}\")\n",
        "\n",
        "    # Train and evaluate model\n",
        "    model, metrics = train_and_evaluate_model(X_train, y_train, X_test, y_test, label_encoder)\n",
        "\n",
        "    # Save model\n",
        "    model.save_model('xgboost_balanced_dataset_b.json')\n",
        "    print(\"\\nModel saved as 'xgboost_balanced_dataset_b.json'\")\n",
        "\n",
        "    print(\"\\nAnalysis complete!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "00eGSWR0uthQ",
        "outputId": "70cab766-68fc-4ad7-bba3-525c91543122"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounting Google Drive...\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Google Drive mounted successfully.\n",
            "\n",
            "Searching for balanced_10k_test.csv in /content/drive/My Drive/CICIDS2017_improved...\n",
            "Files in directory:\n",
            "  - monday.csv\n",
            "  - tuesday.csv\n",
            "  - wednesday.csv\n",
            "  - thursday.csv\n",
            "  - friday.csv\n",
            "  - combined_dataset.csv\n",
            "  - balanced_multiclass_10k.csv\n",
            "  - train.csv\n",
            "  - metrics_by_class.png\n",
            "  - class_metrics.csv\n",
            "  - prediction_results.csv\n",
            "  - confusion_matrix.csv\n",
            "  - confusion_metrics_by_class.png\n",
            "  - fixed_train.csv\n",
            "  - fixed_test.csv\n",
            "  - enhanced_train.csv\n",
            "  - plots_multiclass\n",
            "  - multiclass_metrics.csv\n",
            "\n",
            "Possible balanced dataset files found:\n",
            "  - balanced_multiclass_10k.csv\n",
            "\n",
            "Using file: balanced_multiclass_10k.csv\n",
            "Loading balanced 10k test dataset from /content/drive/My Drive/CICIDS2017_improved/balanced_multiclass_10k.csv...\n",
            "Dataset loaded successfully. Shape: (10000, 91)\n",
            "\n",
            "Splitting dataset into 70% training and 30% testing...\n",
            "Training set: 7000 samples\n",
            "Test set: 3000 samples\n",
            "\n",
            "Preprocessing data:\n",
            "Initial shape: (7000, 91)\n",
            "Label distribution: {'BENIGN': 2794, 'Portscan': 1187, 'DDoS': 957, 'Infiltration - Portscan': 845, 'Botnet': 521, 'Web Attack - XSS - Attempted': 433, 'Web Attack - Brute Force - Attempted': 69, 'Botnet - Attempted': 68, 'Web Attack - Brute Force': 47, 'Infiltration - Attempted': 28, 'Infiltration': 26, 'Web Attack - XSS': 13, 'Web Attack - SQL Injection': 9, 'Web Attack - SQL Injection - Attempted': 3}\n",
            "\n",
            "Label encoding mapping:\n",
            "  0: BENIGN\n",
            "  1: Botnet\n",
            "  2: Botnet - Attempted\n",
            "  3: DDoS\n",
            "  4: Infiltration\n",
            "  5: Infiltration - Attempted\n",
            "  6: Infiltration - Portscan\n",
            "  7: Portscan\n",
            "  8: Web Attack - Brute Force\n",
            "  9: Web Attack - Brute Force - Attempted\n",
            "  10: Web Attack - SQL Injection\n",
            "  11: Web Attack - SQL Injection - Attempted\n",
            "  12: Web Attack - XSS\n",
            "  13: Web Attack - XSS - Attempted\n",
            "Number of numeric columns after conversion: 82\n",
            "NaN values before handling: 0\n",
            "NaN values after handling: 0\n",
            "Preprocessed dataset shape: (7000, 83)\n",
            "Number of unique classes: 14\n",
            "\n",
            "Preprocessing data:\n",
            "Initial shape: (3000, 91)\n",
            "Label distribution: {'BENIGN': 1196, 'Portscan': 500, 'DDoS': 378, 'Infiltration - Portscan': 362, 'Web Attack - XSS - Attempted': 222, 'Botnet': 215, 'Botnet - Attempted': 32, 'Web Attack - Brute Force - Attempted': 31, 'Web Attack - Brute Force': 26, 'Infiltration - Attempted': 17, 'Infiltration': 10, 'Web Attack - XSS': 5, 'Web Attack - SQL Injection': 4, 'Web Attack - SQL Injection - Attempted': 2}\n",
            "Number of numeric columns after conversion: 82\n",
            "NaN values before handling: 0\n",
            "NaN values after handling: 0\n",
            "Preprocessed dataset shape: (3000, 83)\n",
            "Number of unique classes: 14\n",
            "\n",
            "Training features shape: (7000, 82)\n",
            "Training target shape: (7000,)\n",
            "Test features shape: (3000, 82)\n",
            "Test target shape: (3000,)\n",
            "\n",
            "Training XGBoost model...\n",
            "Number of classes: 14\n",
            "\n",
            "Evaluating model...\n",
            "\n",
            "Model               Accuracy    F1 Score    False PostiveFalse NegativeDetection   \n",
            "Prediction time: 18.44 μs/sample\n",
            "XGBoost, Dataset B  0.967000000.9651142999          99          0.96700000\n",
            "\n",
            "Classification Report:\n",
            "                                        precision    recall  f1-score   support\n",
            "\n",
            "                                BENIGN       0.99      1.00      1.00      1196\n",
            "                                Botnet       1.00      1.00      1.00       215\n",
            "                    Botnet - Attempted       1.00      1.00      1.00        32\n",
            "                                  DDoS       1.00      1.00      1.00       378\n",
            "                          Infiltration       1.00      0.60      0.75        10\n",
            "              Infiltration - Attempted       0.94      0.88      0.91        17\n",
            "               Infiltration - Portscan       0.94      0.93      0.93       362\n",
            "                              Portscan       0.95      0.95      0.95       500\n",
            "              Web Attack - Brute Force       1.00      1.00      1.00        26\n",
            "  Web Attack - Brute Force - Attempted       0.28      0.16      0.20        31\n",
            "            Web Attack - SQL Injection       1.00      1.00      1.00         4\n",
            "Web Attack - SQL Injection - Attempted       0.00      0.00      0.00         2\n",
            "                      Web Attack - XSS       1.00      0.80      0.89         5\n",
            "          Web Attack - XSS - Attempted       0.88      0.94      0.91       222\n",
            "\n",
            "                              accuracy                           0.97      3000\n",
            "                             macro avg       0.86      0.80      0.82      3000\n",
            "                          weighted avg       0.96      0.97      0.97      3000\n",
            "\n",
            "Confusion matrix saved as 'confusion_matrix_balanced_dataset_b.png'\n",
            "Feature importance plot saved as 'feature_importance_balanced_dataset_b.png'\n",
            "\n",
            "Model saved as 'xgboost_balanced_dataset_b.json'\n",
            "\n",
            "Analysis complete!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x1000 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import time\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "import xgboost as xgb\n",
        "import warnings\n",
        "import os\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def calculate_detailed_metrics(y_true, y_pred, model_name):\n",
        "    \"\"\"\n",
        "    Calculate detailed metrics for model evaluation.\n",
        "\n",
        "    Args:\n",
        "        y_true: True labels\n",
        "        y_pred: Predicted labels\n",
        "        model_name: Name of the model for display\n",
        "\n",
        "    Returns:\n",
        "        Dictionary of metrics\n",
        "    \"\"\"\n",
        "    # Basic metrics\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "\n",
        "    # Confusion matrix to get FP and FN\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    # For multi-class, calculate total false positives and false negatives\n",
        "    false_positives = 0\n",
        "    false_negatives = 0\n",
        "\n",
        "    # Loop through each class\n",
        "    for i in range(len(cm)):\n",
        "        # False positives are sum of column i minus value at position (i,i)\n",
        "        false_positives += sum(cm[:,i]) - cm[i,i]\n",
        "        # False negatives are sum of row i minus value at position (i,i)\n",
        "        false_negatives += sum(cm[i,:]) - cm[i,i]\n",
        "\n",
        "    # Calculate detection rate (True Positives / (True Positives + False Negatives))\n",
        "    # For multiclass, detection is macro-averaged recall\n",
        "    detection = np.sum(np.diag(cm)) / np.sum(cm)\n",
        "\n",
        "    # Print results in tabular format\n",
        "    print(f\"{model_name:<20}{accuracy:.8f}{f1:.8f}{false_positives:<12}{false_negatives:<12}{detection:.8f}\")\n",
        "\n",
        "    # Return metrics dict\n",
        "    return {\n",
        "        'Accuracy': accuracy,\n",
        "        'F1 Score': f1,\n",
        "        'False Positives': false_positives,\n",
        "        'False Negatives': false_negatives,\n",
        "        'Detection': detection\n",
        "    }\n",
        "\n",
        "def preprocess_data(df, scaler=None, label_encoder=None, fit_scaler=False, fit_encoder=False):\n",
        "    \"\"\"\n",
        "    Preprocess the dataset by cleaning, encoding labels, and normalizing features.\n",
        "    \"\"\"\n",
        "    print(\"\\nPreprocessing data:\")\n",
        "    print(f\"Initial shape: {df.shape}\")\n",
        "\n",
        "    # Make a copy to avoid modifying the original\n",
        "    df_processed = df.copy()\n",
        "\n",
        "    # Identify the label column\n",
        "    label_col = 'Label'  # From the CSV info, we know it's called 'Label'\n",
        "    if label_col not in df_processed.columns:\n",
        "        raise ValueError(f\"Label column '{label_col}' not found in the dataset\")\n",
        "\n",
        "    # Check label distribution\n",
        "    label_counts = df_processed[label_col].value_counts()\n",
        "    print(f\"Label distribution: {label_counts.to_dict()}\")\n",
        "\n",
        "    # Keep the original label\n",
        "    original_label = df_processed[label_col].copy()\n",
        "\n",
        "    # Label encode the attack types\n",
        "    if fit_encoder:\n",
        "        label_encoder = LabelEncoder()\n",
        "        encoded_labels = label_encoder.fit_transform(original_label)\n",
        "\n",
        "        # Display mapping\n",
        "        print(\"\\nLabel encoding mapping:\")\n",
        "        for i, label in enumerate(label_encoder.classes_):\n",
        "            print(f\"  {i}: {label}\")\n",
        "    else:\n",
        "        encoded_labels = label_encoder.transform(original_label)\n",
        "\n",
        "    # Add encoded labels column\n",
        "    df_processed['Label_Encoded'] = encoded_labels\n",
        "\n",
        "    # Remove identifiers and other non-feature columns\n",
        "    columns_to_drop = [\n",
        "        'id', 'Flow ID', 'Src IP', 'Src Port', 'Dst IP', 'Dst Port',\n",
        "        'Protocol', 'Timestamp', 'Label'\n",
        "    ]\n",
        "\n",
        "    # Only drop columns that exist in the dataframe\n",
        "    columns_to_drop = [col for col in columns_to_drop if col in df_processed.columns]\n",
        "    df_cleaned = df_processed.drop(columns=columns_to_drop, errors='ignore')\n",
        "\n",
        "    # Convert all columns to numeric except Label_Encoded\n",
        "    numeric_cols = []\n",
        "    for col in df_cleaned.columns:\n",
        "        if col != 'Label_Encoded':\n",
        "            try:\n",
        "                df_cleaned[col] = pd.to_numeric(df_cleaned[col], errors='coerce')\n",
        "                numeric_cols.append(col)\n",
        "            except Exception as e:\n",
        "                print(f\"Error converting column {col} to numeric: {e}\")\n",
        "                df_cleaned = df_cleaned.drop(columns=[col])\n",
        "\n",
        "    # Display info about numeric columns\n",
        "    print(f\"Number of numeric columns after conversion: {len(numeric_cols)}\")\n",
        "\n",
        "    # Handle NaN values\n",
        "    print(f\"NaN values before handling: {df_cleaned[numeric_cols].isna().sum().sum()}\")\n",
        "\n",
        "    # Replace infinity values with NaN\n",
        "    df_cleaned = df_cleaned.replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "    # Handle outliers with quantile clipping\n",
        "    for col in numeric_cols:\n",
        "        upper_limit = df_cleaned[col].quantile(0.99)\n",
        "        lower_limit = df_cleaned[col].quantile(0.01)\n",
        "        df_cleaned[col] = np.clip(df_cleaned[col], lower_limit, upper_limit)\n",
        "\n",
        "    # Fill NaN values with column means\n",
        "    for col in numeric_cols:\n",
        "        if df_cleaned[col].isna().any():\n",
        "            col_mean = df_cleaned[col].mean()\n",
        "            df_cleaned[col] = df_cleaned[col].fillna(col_mean)\n",
        "\n",
        "    print(f\"NaN values after handling: {df_cleaned[numeric_cols].isna().sum().sum()}\")\n",
        "\n",
        "    # Apply min-max scaling to numeric columns\n",
        "    features = df_cleaned[numeric_cols]\n",
        "    labels = df_cleaned['Label_Encoded']\n",
        "\n",
        "    # Apply scaling\n",
        "    if fit_scaler:\n",
        "        scaler = MinMaxScaler()\n",
        "        scaled_features = scaler.fit_transform(features)\n",
        "    else:\n",
        "        scaled_features = scaler.transform(features)\n",
        "\n",
        "    # Create a new dataframe with scaled features\n",
        "    scaled_df = pd.DataFrame(scaled_features, columns=features.columns)\n",
        "\n",
        "    # Add back the encoded label column\n",
        "    scaled_df['Label_Encoded'] = labels.values\n",
        "\n",
        "    print(f\"Preprocessed dataset shape: {scaled_df.shape}\")\n",
        "    print(f\"Number of unique classes: {scaled_df['Label_Encoded'].nunique()}\")\n",
        "\n",
        "    # Return processed dataframe and optionally scaler and encoder\n",
        "    if fit_scaler or fit_encoder:\n",
        "        return scaled_df, scaler, label_encoder\n",
        "    else:\n",
        "        return scaled_df\n",
        "\n",
        "def train_and_evaluate_model(X_train, y_train, X_test, y_test, label_encoder):\n",
        "    \"\"\"\n",
        "    Train an XGBoost model and evaluate its performance\n",
        "    \"\"\"\n",
        "    print(\"\\nTraining XGBoost model...\")\n",
        "\n",
        "    # Count number of classes\n",
        "    num_classes = len(np.unique(y_train))\n",
        "    print(f\"Number of classes: {num_classes}\")\n",
        "\n",
        "    # Set parameters\n",
        "    params = {\n",
        "        'objective': 'multi:softprob',\n",
        "        'num_class': num_classes,\n",
        "        'eta': 0.1,\n",
        "        'max_depth': 6,\n",
        "        'min_child_weight': 1,\n",
        "        'subsample': 0.8,\n",
        "        'colsample_bytree': 0.8,\n",
        "        'tree_method': 'hist',\n",
        "        'eval_metric': 'mlogloss',\n",
        "        'use_label_encoder': False\n",
        "    }\n",
        "\n",
        "    # Create and train the model\n",
        "    model = xgb.XGBClassifier(**params)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate the model\n",
        "    print(\"\\nEvaluating model...\")\n",
        "\n",
        "    # Print metrics header\n",
        "    print(f\"\\n{'Model':<20}{'Accuracy':<12}{'F1 Score':<12}{'False Postive':<12}{'False Negative':<12}{'Detection':<12}\")\n",
        "\n",
        "    # Predict and time\n",
        "    start_time = time.time()\n",
        "    y_pred = model.predict(X_test)\n",
        "    end_time = time.time()\n",
        "\n",
        "    # Calculate prediction time per sample\n",
        "    prediction_time = (end_time - start_time) * 1000000 / len(X_test)\n",
        "    print(f\"Prediction time: {prediction_time:.2f} μs/sample\")\n",
        "\n",
        "    # Calculate detailed metrics\n",
        "    detailed_metrics = calculate_detailed_metrics(y_test, y_pred, \"XGBoost, Dataset A, 10k\")\n",
        "\n",
        "    # Generate classification report\n",
        "    unique_test_classes = np.unique(y_test)\n",
        "    class_names = [label_encoder.classes_[i] for i in unique_test_classes]\n",
        "\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test, y_pred, labels=unique_test_classes, target_names=class_names))\n",
        "\n",
        "    # Create confusion matrix visualization\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=class_names,\n",
        "                yticklabels=class_names)\n",
        "    plt.title('Confusion Matrix - Dataset A, 10k')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.xticks(rotation=90)\n",
        "    plt.yticks(rotation=0)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('confusion_matrix_dataset_a_10k.png')\n",
        "    plt.close()\n",
        "    print(\"Confusion matrix saved as 'confusion_matrix_dataset_a_10k.png'\")\n",
        "\n",
        "    # Create feature importance plot\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    xgb.plot_importance(model, max_num_features=20)\n",
        "    plt.title('Feature Importance - Dataset A, 10k')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('feature_importance_dataset_a_10k.png')\n",
        "    plt.close()\n",
        "    print(\"Feature importance plot saved as 'feature_importance_dataset_a_10k.png'\")\n",
        "\n",
        "    return model, detailed_metrics\n",
        "\n",
        "def main():\n",
        "    # Mount Google Drive if running in Colab\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        print(\"Mounting Google Drive...\")\n",
        "        drive.mount('/content/drive')\n",
        "        print(\"Google Drive mounted successfully.\")\n",
        "    except:\n",
        "        print(\"Not running in Colab or Drive already mounted.\")\n",
        "\n",
        "    # Set base directory\n",
        "    base_dir = \"/content/drive/My Drive/CICIDS2017_improved\"\n",
        "\n",
        "    # Specify the dataset file\n",
        "    dataset_name = \"dataset_A_10k.csv\"\n",
        "    dataset_path = os.path.join(base_dir, dataset_name)\n",
        "\n",
        "    # Check if the file exists\n",
        "    if not os.path.exists(dataset_path):\n",
        "        print(f\"File not found: {dataset_path}\")\n",
        "        print(\"Files in directory:\")\n",
        "        if os.path.exists(base_dir):\n",
        "            for file in os.listdir(base_dir):\n",
        "                print(f\"  - {file}\")\n",
        "        dataset_path = input(f\"Please enter the full path to {dataset_name}: \")\n",
        "\n",
        "    # Load the dataset\n",
        "    print(f\"Loading dataset from {dataset_path}...\")\n",
        "    try:\n",
        "        df = pd.read_csv(dataset_path)\n",
        "        print(f\"Dataset loaded successfully. Shape: {df.shape}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading dataset: {e}\")\n",
        "        return\n",
        "\n",
        "    # Split data into training and testing sets\n",
        "    print(\"\\nSplitting dataset into 70% training and 30% testing...\")\n",
        "    train_df, test_df = train_test_split(df, test_size=0.3, random_state=42)\n",
        "    print(f\"Training set: {len(train_df)} samples\")\n",
        "    print(f\"Test set: {len(test_df)} samples\")\n",
        "\n",
        "    # Preprocess training data\n",
        "    train_processed, scaler, label_encoder = preprocess_data(\n",
        "        train_df, fit_scaler=True, fit_encoder=True\n",
        "    )\n",
        "\n",
        "    # Preprocess test data\n",
        "    test_processed = preprocess_data(\n",
        "        test_df, scaler=scaler, label_encoder=label_encoder,\n",
        "        fit_scaler=False, fit_encoder=False\n",
        "    )\n",
        "\n",
        "    # Prepare features and labels\n",
        "    X_train = train_processed.drop('Label_Encoded', axis=1)\n",
        "    y_train = train_processed['Label_Encoded']\n",
        "    X_test = test_processed.drop('Label_Encoded', axis=1)\n",
        "    y_test = test_processed['Label_Encoded']\n",
        "\n",
        "    print(f\"\\nTraining features shape: {X_train.shape}\")\n",
        "    print(f\"Training target shape: {y_train.shape}\")\n",
        "    print(f\"Test features shape: {X_test.shape}\")\n",
        "    print(f\"Test target shape: {y_test.shape}\")\n",
        "\n",
        "    # Train and evaluate model\n",
        "    model, metrics = train_and_evaluate_model(X_train, y_train, X_test, y_test, label_encoder)\n",
        "\n",
        "    # Save model\n",
        "    model.save_model('xgboost_dataset_a_10k.json')\n",
        "    print(\"\\nModel saved as 'xgboost_dataset_a_10k.json'\")\n",
        "\n",
        "    print(\"\\nAnalysis complete!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DWe40P7dwSK4",
        "outputId": "f3bec429-3a0a-4995-e855-97c467abae4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounting Google Drive...\n",
            "Mounted at /content/drive\n",
            "Google Drive mounted successfully.\n",
            "Loading dataset from /content/drive/My Drive/CICIDS2017_improved/dataset_A_10k.csv...\n",
            "Dataset loaded successfully. Shape: (10000, 91)\n",
            "\n",
            "Splitting dataset into 70% training and 30% testing...\n",
            "Training set: 7000 samples\n",
            "Test set: 3000 samples\n",
            "\n",
            "Preprocessing data:\n",
            "Initial shape: (7000, 91)\n",
            "Label distribution: {'BENIGN': 3461, 'Portscan': 1116, 'DoS Hulk': 1067, 'DDoS': 654, 'Infiltration - Portscan': 492, 'DoS GoldenEye': 55, 'DoS Slowloris': 28, 'Botnet - Attempted': 26, 'FTP-Patator': 25, 'SSH-Patator': 19, 'DoS Slowhttptest - Attempted': 18, 'DoS Slowhttptest': 12, 'Web Attack - Brute Force - Attempted': 10, 'DoS Slowloris - Attempted': 5, 'Web Attack - XSS - Attempted': 5, 'Web Attack - Brute Force': 3, 'Botnet': 2, 'DoS Hulk - Attempted': 1, 'Infiltration': 1}\n",
            "\n",
            "Label encoding mapping:\n",
            "  0: BENIGN\n",
            "  1: Botnet\n",
            "  2: Botnet - Attempted\n",
            "  3: DDoS\n",
            "  4: DoS GoldenEye\n",
            "  5: DoS Hulk\n",
            "  6: DoS Hulk - Attempted\n",
            "  7: DoS Slowhttptest\n",
            "  8: DoS Slowhttptest - Attempted\n",
            "  9: DoS Slowloris\n",
            "  10: DoS Slowloris - Attempted\n",
            "  11: FTP-Patator\n",
            "  12: Infiltration\n",
            "  13: Infiltration - Portscan\n",
            "  14: Portscan\n",
            "  15: SSH-Patator\n",
            "  16: Web Attack - Brute Force\n",
            "  17: Web Attack - Brute Force - Attempted\n",
            "  18: Web Attack - XSS - Attempted\n",
            "Number of numeric columns after conversion: 82\n",
            "NaN values before handling: 0\n",
            "NaN values after handling: 0\n",
            "Preprocessed dataset shape: (7000, 83)\n",
            "Number of unique classes: 19\n",
            "\n",
            "Preprocessing data:\n",
            "Initial shape: (3000, 91)\n",
            "Label distribution: {'BENIGN': 1539, 'Portscan': 465, 'DoS Hulk': 429, 'DDoS': 269, 'Infiltration - Portscan': 208, 'DoS GoldenEye': 22, 'FTP-Patator': 12, 'DoS Slowhttptest - Attempted': 11, 'Botnet - Attempted': 11, 'DoS Slowloris - Attempted': 7, 'SSH-Patator': 6, 'DoS Slowloris': 6, 'Web Attack - Brute Force - Attempted': 5, 'Web Attack - XSS - Attempted': 5, 'Botnet': 3, 'DoS Slowhttptest': 2}\n",
            "Number of numeric columns after conversion: 82\n",
            "NaN values before handling: 0\n",
            "NaN values after handling: 0\n",
            "Preprocessed dataset shape: (3000, 83)\n",
            "Number of unique classes: 16\n",
            "\n",
            "Training features shape: (7000, 82)\n",
            "Training target shape: (7000,)\n",
            "Test features shape: (3000, 82)\n",
            "Test target shape: (3000,)\n",
            "\n",
            "Training XGBoost model...\n",
            "Number of classes: 19\n",
            "\n",
            "Evaluating model...\n",
            "\n",
            "Model               Accuracy    F1 Score    False PostiveFalse NegativeDetection   \n",
            "Prediction time: 19.08 μs/sample\n",
            "XGBoost, Dataset A, 10k0.982333330.9819311953          53          0.98233333\n",
            "\n",
            "Classification Report:\n",
            "                                      precision    recall  f1-score   support\n",
            "\n",
            "                              BENIGN       0.99      1.00      1.00      1539\n",
            "                              Botnet       1.00      0.67      0.80         3\n",
            "                  Botnet - Attempted       0.92      1.00      0.96        11\n",
            "                                DDoS       1.00      1.00      1.00       269\n",
            "                       DoS GoldenEye       1.00      1.00      1.00        22\n",
            "                            DoS Hulk       1.00      1.00      1.00       429\n",
            "                    DoS Slowhttptest       1.00      1.00      1.00         2\n",
            "        DoS Slowhttptest - Attempted       1.00      0.91      0.95        11\n",
            "                       DoS Slowloris       1.00      1.00      1.00         6\n",
            "           DoS Slowloris - Attempted       1.00      0.86      0.92         7\n",
            "                         FTP-Patator       1.00      1.00      1.00        12\n",
            "             Infiltration - Portscan       0.92      0.88      0.90       208\n",
            "                            Portscan       0.96      0.96      0.96       465\n",
            "                         SSH-Patator       1.00      1.00      1.00         6\n",
            "Web Attack - Brute Force - Attempted       0.40      0.40      0.40         5\n",
            "        Web Attack - XSS - Attempted       0.33      0.20      0.25         5\n",
            "\n",
            "                            accuracy                           0.98      3000\n",
            "                           macro avg       0.91      0.87      0.88      3000\n",
            "                        weighted avg       0.98      0.98      0.98      3000\n",
            "\n",
            "Confusion matrix saved as 'confusion_matrix_dataset_a_10k.png'\n",
            "Feature importance plot saved as 'feature_importance_dataset_a_10k.png'\n",
            "\n",
            "Model saved as 'xgboost_dataset_a_10k.json'\n",
            "\n",
            "Analysis complete!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x1000 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import time\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
        "import xgboost as xgb\n",
        "import warnings\n",
        "import os\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def calculate_detailed_metrics(y_true, y_pred, model_name):\n",
        "    \"\"\"\n",
        "    Calculate detailed metrics for model evaluation.\n",
        "    \"\"\"\n",
        "    # Basic metrics\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "\n",
        "    # Confusion matrix to get FP and FN\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    # For multi-class, calculate total false positives and false negatives\n",
        "    false_positives = 0\n",
        "    false_negatives = 0\n",
        "\n",
        "    # Loop through each class\n",
        "    for i in range(len(cm)):\n",
        "        # False positives are sum of column i minus value at position (i,i)\n",
        "        false_positives += sum(cm[:,i]) - cm[i,i]\n",
        "        # False negatives are sum of row i minus value at position (i,i)\n",
        "        false_negatives += sum(cm[i,:]) - cm[i,i]\n",
        "\n",
        "    # Calculate detection rate (True Positives / (True Positives + False Negatives))\n",
        "    detection = np.sum(np.diag(cm)) / np.sum(cm)\n",
        "\n",
        "    # Print results in tabular format\n",
        "    print(f\"{model_name:<35}{accuracy:.8f}{f1:.8f}{false_positives:<12}{false_negatives:<12}{detection:.8f}\")\n",
        "\n",
        "    return {\n",
        "        'Accuracy': accuracy,\n",
        "        'F1 Score': f1,\n",
        "        'False Positives': false_positives,\n",
        "        'False Negatives': false_negatives,\n",
        "        'Detection': detection\n",
        "    }\n",
        "\n",
        "def load_data(file_path):\n",
        "    \"\"\"\n",
        "    Load network flow data from a CSV file.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "        print(f\"Loaded dataset with shape: {df.shape}\")\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading dataset: {e}\")\n",
        "        return None\n",
        "\n",
        "def load_combined_data(base_dir, days=None):\n",
        "    \"\"\"\n",
        "    Load and combine data from multiple day files.\n",
        "\n",
        "    Args:\n",
        "        base_dir: Base directory for data files\n",
        "        days: List of day files to load (e.g., ['monday.csv', 'tuesday.csv'])\n",
        "              If None, load all 5 days\n",
        "\n",
        "    Returns:\n",
        "        Combined DataFrame\n",
        "    \"\"\"\n",
        "    if days is None:\n",
        "        days = [\"monday.csv\", \"tuesday.csv\", \"wednesday.csv\", \"thursday.csv\", \"friday.csv\"]\n",
        "\n",
        "    print(f\"Loading data from days: {days}\")\n",
        "\n",
        "    combined_df_list = []\n",
        "    for day in days:\n",
        "        file_path = os.path.join(base_dir, day)\n",
        "        if os.path.exists(file_path):\n",
        "            df = pd.read_csv(file_path)\n",
        "            print(f\"  - {day}: {df.shape[0]} samples\")\n",
        "            combined_df_list.append(df)\n",
        "        else:\n",
        "            print(f\"  - Warning: {day} not found\")\n",
        "\n",
        "    # Combine all dataframes\n",
        "    combined_df = pd.concat(combined_df_list, ignore_index=True)\n",
        "    print(f\"Combined dataset shape: {combined_df.shape}\")\n",
        "\n",
        "    return combined_df\n",
        "\n",
        "def preprocess_data(df, scaler=None, label_encoder=None, fit_scaler=False, fit_encoder=False, handle_unknown=False):\n",
        "    \"\"\"\n",
        "    Preprocess the dataset by cleaning, encoding labels, and normalizing features.\n",
        "    \"\"\"\n",
        "    print(f\"\\nPreprocessing dataset with shape: {df.shape}\")\n",
        "\n",
        "    # Make a copy to avoid modifying the original\n",
        "    df_processed = df.copy()\n",
        "\n",
        "    # Identify the label column\n",
        "    label_col = 'Label'\n",
        "    if label_col not in df_processed.columns:\n",
        "        raise ValueError(f\"Label column '{label_col}' not found in the dataset\")\n",
        "\n",
        "    # Check label distribution\n",
        "    label_counts = df_processed[label_col].value_counts()\n",
        "    print(f\"Label distribution: {len(label_counts)} unique classes\")\n",
        "    print(f\"Top 5 classes: {dict(label_counts.head(5))}\")\n",
        "\n",
        "    # Keep the original label\n",
        "    original_label = df_processed[label_col].copy()\n",
        "\n",
        "    # Label encode the attack types\n",
        "    if fit_encoder:\n",
        "        label_encoder = LabelEncoder()\n",
        "        encoded_labels = label_encoder.fit_transform(original_label)\n",
        "\n",
        "        # Display mapping\n",
        "        print(\"\\nLabel encoding mapping:\")\n",
        "        for i, label in enumerate(label_encoder.classes_):\n",
        "            print(f\"  {i}: {label}\")\n",
        "    else:\n",
        "        if handle_unknown:\n",
        "            # Handle unknown labels in test set\n",
        "            unknown_labels = set(original_label) - set(label_encoder.classes_)\n",
        "            if unknown_labels:\n",
        "                print(f\"Found {len(unknown_labels)} unknown labels in test data\")\n",
        "                # Map unknown labels to 'UNKNOWN'\n",
        "                df_processed[label_col] = df_processed[label_col].apply(\n",
        "                    lambda x: x if x in label_encoder.classes_ else 'UNKNOWN'\n",
        "                )\n",
        "\n",
        "                # Update original_label\n",
        "                original_label = df_processed[label_col].copy()\n",
        "\n",
        "                # Add 'UNKNOWN' to classes if not already present\n",
        "                if 'UNKNOWN' not in label_encoder.classes_:\n",
        "                    label_encoder.classes_ = np.append(label_encoder.classes_, 'UNKNOWN')\n",
        "\n",
        "        try:\n",
        "            encoded_labels = label_encoder.transform(original_label)\n",
        "        except Exception as e:\n",
        "            print(f\"Error transforming labels: {e}\")\n",
        "            raise\n",
        "\n",
        "    # Add encoded labels column\n",
        "    df_processed['Label_Encoded'] = encoded_labels\n",
        "\n",
        "    # Remove identifiers and other non-feature columns\n",
        "    columns_to_drop = [\n",
        "        'id', 'Flow ID', 'Src IP', 'Src Port', 'Dst IP', 'Dst Port',\n",
        "        'Protocol', 'Timestamp', 'Label'\n",
        "    ]\n",
        "\n",
        "    # Only drop columns that exist in the dataframe\n",
        "    columns_to_drop = [col for col in columns_to_drop if col in df_processed.columns]\n",
        "    df_cleaned = df_processed.drop(columns=columns_to_drop, errors='ignore')\n",
        "\n",
        "    # Convert all columns to numeric except Label_Encoded\n",
        "    numeric_cols = []\n",
        "    for col in df_cleaned.columns:\n",
        "        if col != 'Label_Encoded':\n",
        "            try:\n",
        "                df_cleaned[col] = pd.to_numeric(df_cleaned[col], errors='coerce')\n",
        "                numeric_cols.append(col)\n",
        "            except Exception as e:\n",
        "                print(f\"Error converting column {col} to numeric: {e}\")\n",
        "                df_cleaned = df_cleaned.drop(columns=[col])\n",
        "\n",
        "    # Handle NaN values\n",
        "    print(f\"NaN values before handling: {df_cleaned[numeric_cols].isna().sum().sum()}\")\n",
        "\n",
        "    # Replace infinity values with NaN\n",
        "    df_cleaned = df_cleaned.replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "    # Fill NaN values with column means\n",
        "    for col in numeric_cols:\n",
        "        if df_cleaned[col].isna().any():\n",
        "            col_mean = df_cleaned[col].mean()\n",
        "            df_cleaned[col] = df_cleaned[col].fillna(col_mean)\n",
        "\n",
        "    print(f\"NaN values after handling: {df_cleaned[numeric_cols].isna().sum().sum()}\")\n",
        "\n",
        "    # Apply scaling\n",
        "    features = df_cleaned[numeric_cols]\n",
        "    labels = df_cleaned['Label_Encoded']\n",
        "\n",
        "    if fit_scaler:\n",
        "        scaler = MinMaxScaler()\n",
        "        scaled_features = scaler.fit_transform(features)\n",
        "    else:\n",
        "        scaled_features = scaler.transform(features)\n",
        "\n",
        "    # Create a new dataframe with scaled features\n",
        "    scaled_df = pd.DataFrame(scaled_features, columns=features.columns)\n",
        "\n",
        "    # Add back the encoded label column\n",
        "    scaled_df['Label_Encoded'] = labels.values\n",
        "\n",
        "    print(f\"Preprocessed dataset shape: {scaled_df.shape}\")\n",
        "    print(f\"Number of unique classes: {scaled_df['Label_Encoded'].nunique()}\")\n",
        "\n",
        "    if fit_scaler or fit_encoder:\n",
        "        return scaled_df, scaler, label_encoder\n",
        "    else:\n",
        "        return scaled_df\n",
        "\n",
        "def train_model(X_train, y_train, num_classes):\n",
        "    \"\"\"\n",
        "    Train an XGBoost multiclass classifier.\n",
        "    \"\"\"\n",
        "    print(\"\\nTraining XGBoost model...\")\n",
        "    print(f\"Training data shape: {X_train.shape}\")\n",
        "    print(f\"Number of classes: {num_classes}\")\n",
        "\n",
        "    # Set parameters\n",
        "    params = {\n",
        "        'objective': 'multi:softprob',\n",
        "        'num_class': num_classes,\n",
        "        'eta': 0.1,\n",
        "        'max_depth': 6,\n",
        "        'min_child_weight': 1,\n",
        "        'subsample': 0.8,\n",
        "        'colsample_bytree': 0.8,\n",
        "        'tree_method': 'hist',\n",
        "        'eval_metric': 'mlogloss',\n",
        "        'use_label_encoder': False\n",
        "    }\n",
        "\n",
        "    # Create and train model\n",
        "    model = xgb.XGBClassifier(**params)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    return model\n",
        "\n",
        "def evaluate_model(model, X_test, y_test, label_encoder, model_name):\n",
        "    \"\"\"\n",
        "    Evaluate model performance on test data.\n",
        "    \"\"\"\n",
        "    print(f\"\\nEvaluating model: {model_name}\")\n",
        "    print(f\"Test data shape: {X_test.shape}\")\n",
        "\n",
        "    # Measure prediction time\n",
        "    start_time = time.time()\n",
        "    y_pred = model.predict(X_test)\n",
        "    end_time = time.time()\n",
        "\n",
        "    # Calculate prediction time\n",
        "    prediction_time = (end_time - start_time) * 1000000 / len(X_test)\n",
        "    print(f\"Prediction time: {prediction_time:.2f} μs/sample\")\n",
        "\n",
        "    # Calculate detailed metrics\n",
        "    metrics = calculate_detailed_metrics(y_test, y_pred, model_name)\n",
        "\n",
        "    # Classification report\n",
        "    unique_classes = np.unique(y_test)\n",
        "    class_names = [label_encoder.classes_[i] for i in unique_classes]\n",
        "\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test, y_pred, labels=unique_classes, target_names=class_names))\n",
        "\n",
        "    # Create confusion matrix visualization\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=class_names,\n",
        "                yticklabels=class_names)\n",
        "    plt.title(f'Confusion Matrix - {model_name}')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.xticks(rotation=90)\n",
        "    plt.yticks(rotation=0)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Create safe filename\n",
        "    filename = model_name.replace('[', '').replace(']', '').replace(' ', '_').lower()\n",
        "    plt.savefig(f'confusion_matrix_{filename}.png')\n",
        "    plt.close()\n",
        "    print(f\"Confusion matrix saved as 'confusion_matrix_{filename}.png'\")\n",
        "\n",
        "    return metrics\n",
        "\n",
        "def main():\n",
        "    # Mount Google Drive if running in Colab\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        print(\"Mounting Google Drive...\")\n",
        "        drive.mount('/content/drive')\n",
        "        print(\"Google Drive mounted successfully.\")\n",
        "    except:\n",
        "        print(\"Not running in Colab or Drive already mounted.\")\n",
        "\n",
        "    # Base directory\n",
        "    base_dir = \"/content/drive/My Drive/CICIDS2017_improved\"\n",
        "\n",
        "    # Define all test files\n",
        "    dataset_a_path = os.path.join(base_dir, \"dataset_A_10k.csv\")\n",
        "    dataset_b_path = os.path.join(base_dir, \"balanced_10k_test.csv\")\n",
        "\n",
        "    # Check if files exist\n",
        "    for path in [dataset_a_path, dataset_b_path]:\n",
        "        if not os.path.exists(path):\n",
        "            print(f\"File not found: {path}\")\n",
        "\n",
        "    # Print files in directory if needed\n",
        "    print(\"\\nFiles in directory:\")\n",
        "    if os.path.exists(base_dir):\n",
        "        files = os.listdir(base_dir)\n",
        "        for file in files:\n",
        "            print(f\"  - {file}\")\n",
        "\n",
        "    # Print metrics header\n",
        "    print(\"\\n\" + \"=\"*100)\n",
        "    print(f\"{'Model':<35}{'Accuracy':<12}{'F1 Score':<12}{'False Postive':<12}{'False Negative':<12}{'Detection':<12}\")\n",
        "    print(\"=\"*100)\n",
        "\n",
        "    # Scenario 1: Train on 5 days, test on Dataset A 10k\n",
        "    print(\"\\n\\nScenario 1: Train on 5 days, test on Dataset A 10k\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    # Load training data (all 5 days)\n",
        "    train_df_5days = load_combined_data(base_dir)\n",
        "\n",
        "    # Load testing data (Dataset A 10k)\n",
        "    test_a_df = load_data(dataset_a_path)\n",
        "\n",
        "    if train_df_5days is not None and test_a_df is not None:\n",
        "        # Preprocess training data\n",
        "        train_processed, scaler, label_encoder = preprocess_data(\n",
        "            train_df_5days, fit_scaler=True, fit_encoder=True\n",
        "        )\n",
        "\n",
        "        # Preprocess test data\n",
        "        test_a_processed = preprocess_data(\n",
        "            test_a_df, scaler=scaler, label_encoder=label_encoder,\n",
        "            fit_scaler=False, fit_encoder=False, handle_unknown=True\n",
        "        )\n",
        "\n",
        "        # Prepare features and labels\n",
        "        X_train = train_processed.drop('Label_Encoded', axis=1)\n",
        "        y_train = train_processed['Label_Encoded']\n",
        "        X_test = test_a_processed.drop('Label_Encoded', axis=1)\n",
        "        y_test = test_a_processed['Label_Encoded']\n",
        "\n",
        "        # Train model\n",
        "        model_5days = train_model(X_train, y_train, len(label_encoder.classes_))\n",
        "\n",
        "        # Evaluate on Dataset A\n",
        "        metrics_a = evaluate_model(\n",
        "            model_5days, X_test, y_test, label_encoder,\n",
        "            \"XGBoost [Trained on 5 days] - Dataset A\"\n",
        "        )\n",
        "\n",
        "        # Save model\n",
        "        model_5days.save_model('xgboost_5days.json')\n",
        "        print(\"Model saved as 'xgboost_5days.json'\")\n",
        "\n",
        "    # Scenario 2: Train on 5 days, test on Dataset B 10k\n",
        "    print(\"\\n\\nScenario 2: Train on 5 days, test on Dataset B 10k\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    # Load testing data (Dataset B 10k)\n",
        "    test_b_df = load_data(dataset_b_path)\n",
        "\n",
        "    if train_df_5days is not None and test_b_df is not None:\n",
        "        # Use the same model and preprocessing from Scenario 1\n",
        "\n",
        "        # Preprocess test data\n",
        "        test_b_processed = preprocess_data(\n",
        "            test_b_df, scaler=scaler, label_encoder=label_encoder,\n",
        "            fit_scaler=False, fit_encoder=False, handle_unknown=True\n",
        "        )\n",
        "\n",
        "        # Prepare features and labels\n",
        "        X_test = test_b_processed.drop('Label_Encoded', axis=1)\n",
        "        y_test = test_b_processed['Label_Encoded']\n",
        "\n",
        "        # Evaluate on Dataset B\n",
        "        metrics_b = evaluate_model(\n",
        "            model_5days, X_test, y_test, label_encoder,\n",
        "            \"XGBoost [Trained on 5 days] - Dataset B\"\n",
        "        )\n",
        "\n",
        "    # Scenario 3: Train on 3 days, test on Dataset B 10k\n",
        "    print(\"\\n\\nScenario 3: Train on 3 days, test on Dataset B 10k\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    # Load training data (first 3 days)\n",
        "    train_df_3days = load_combined_data(\n",
        "        base_dir,\n",
        "        days=[\"monday.csv\", \"tuesday.csv\", \"wednesday.csv\"]\n",
        "    )\n",
        "\n",
        "    if train_df_3days is not None and test_b_df is not None:\n",
        "        # Preprocess training data\n",
        "        train_processed, scaler_3days, label_encoder_3days = preprocess_data(\n",
        "            train_df_3days, fit_scaler=True, fit_encoder=True\n",
        "        )\n",
        "\n",
        "        # Preprocess test data\n",
        "        test_b_processed = preprocess_data(\n",
        "            test_b_df, scaler=scaler_3days, label_encoder=label_encoder_3days,\n",
        "            fit_scaler=False, fit_encoder=False, handle_unknown=True\n",
        "        )\n",
        "\n",
        "        # Prepare features and labels\n",
        "        X_train = train_processed.drop('Label_Encoded', axis=1)\n",
        "        y_train = train_processed['Label_Encoded']\n",
        "        X_test = test_b_processed.drop('Label_Encoded', axis=1)\n",
        "        y_test = test_b_processed['Label_Encoded']\n",
        "\n",
        "        # Train model\n",
        "        model_3days = train_model(X_train, y_train, len(label_encoder_3days.classes_))\n",
        "\n",
        "        # Evaluate on Dataset B\n",
        "        metrics_b = evaluate_model(\n",
        "            model_3days, X_test, y_test, label_encoder_3days,\n",
        "            \"XGBoost [Trained on 3 days] - Dataset B\"\n",
        "        )\n",
        "\n",
        "        # Save model\n",
        "        model_3days.save_model('xgboost_3days.json')\n",
        "        print(\"Model saved as 'xgboost_3days.json'\")\n",
        "\n",
        "    # Print summary of all scenarios\n",
        "    print(\"\\n\\nSummary of All Scenarios\")\n",
        "    print(\"=\" * 100)\n",
        "    print(f\"{'Model':<35}{'Accuracy':<12}{'F1 Score':<12}{'False Postive':<12}{'False Negative':<12}{'Detection':<12}\")\n",
        "    print(\"=\" * 100)\n",
        "    # Metrics will be printed by the evaluate_model function\n",
        "\n",
        "    print(\"\\nAnalysis complete!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQf3XGtsHvgK",
        "outputId": "6da9cb43-13f9-42a1-a11d-21e515ae6229"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounting Google Drive...\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Google Drive mounted successfully.\n",
            "\n",
            "Files in directory:\n",
            "  - monday.csv\n",
            "  - tuesday.csv\n",
            "  - wednesday.csv\n",
            "  - thursday.csv\n",
            "  - friday.csv\n",
            "  - combined_dataset.csv\n",
            "  - balanced_multiclass_10k.csv\n",
            "  - train.csv\n",
            "  - metrics_by_class.png\n",
            "  - class_metrics.csv\n",
            "  - prediction_results.csv\n",
            "  - confusion_matrix.csv\n",
            "  - confusion_metrics_by_class.png\n",
            "  - fixed_train.csv\n",
            "  - fixed_test.csv\n",
            "  - enhanced_train.csv\n",
            "  - plots_multiclass\n",
            "  - multiclass_metrics.csv\n",
            "  - balanced_10k_test.csv\n",
            "  - test_thurs_friday_10000.csv\n",
            "  - dataset_A_10k.csv\n",
            "\n",
            "====================================================================================================\n",
            "Model                              Accuracy    F1 Score    False PostiveFalse NegativeDetection   \n",
            "====================================================================================================\n",
            "\n",
            "\n",
            "Scenario 1: Train on 5 days, test on Dataset A 10k\n",
            "--------------------------------------------------------------------------------\n",
            "Loading data from days: ['monday.csv', 'tuesday.csv', 'wednesday.csv', 'thursday.csv', 'friday.csv']\n",
            "  - monday.csv: 371624 samples\n",
            "  - tuesday.csv: 322078 samples\n",
            "  - wednesday.csv: 496641 samples\n",
            "  - thursday.csv: 362076 samples\n",
            "  - friday.csv: 547557 samples\n",
            "Combined dataset shape: (2099976, 91)\n",
            "Loaded dataset with shape: (10000, 91)\n",
            "\n",
            "Preprocessing dataset with shape: (2099976, 91)\n",
            "Label distribution: 27 unique classes\n",
            "Top 5 classes: {'BENIGN': np.int64(1582566), 'Portscan': np.int64(159066), 'DoS Hulk': np.int64(158468), 'DDoS': np.int64(95144), 'Infiltration - Portscan': np.int64(71767)}\n",
            "\n",
            "Label encoding mapping:\n",
            "  0: BENIGN\n",
            "  1: Botnet\n",
            "  2: Botnet - Attempted\n",
            "  3: DDoS\n",
            "  4: DoS GoldenEye\n",
            "  5: DoS GoldenEye - Attempted\n",
            "  6: DoS Hulk\n",
            "  7: DoS Hulk - Attempted\n",
            "  8: DoS Slowhttptest\n",
            "  9: DoS Slowhttptest - Attempted\n",
            "  10: DoS Slowloris\n",
            "  11: DoS Slowloris - Attempted\n",
            "  12: FTP-Patator\n",
            "  13: FTP-Patator - Attempted\n",
            "  14: Heartbleed\n",
            "  15: Infiltration\n",
            "  16: Infiltration - Attempted\n",
            "  17: Infiltration - Portscan\n",
            "  18: Portscan\n",
            "  19: SSH-Patator\n",
            "  20: SSH-Patator - Attempted\n",
            "  21: Web Attack - Brute Force\n",
            "  22: Web Attack - Brute Force - Attempted\n",
            "  23: Web Attack - SQL Injection\n",
            "  24: Web Attack - SQL Injection - Attempted\n",
            "  25: Web Attack - XSS\n",
            "  26: Web Attack - XSS - Attempted\n",
            "NaN values before handling: 0\n",
            "NaN values after handling: 0\n",
            "Preprocessed dataset shape: (2099976, 83)\n",
            "Number of unique classes: 27\n",
            "\n",
            "Preprocessing dataset with shape: (10000, 91)\n",
            "Label distribution: 19 unique classes\n",
            "Top 5 classes: {'BENIGN': np.int64(5000), 'Portscan': np.int64(1581), 'DoS Hulk': np.int64(1496), 'DDoS': np.int64(923), 'Infiltration - Portscan': np.int64(700)}\n",
            "NaN values before handling: 0\n",
            "NaN values after handling: 0\n",
            "Preprocessed dataset shape: (10000, 83)\n",
            "Number of unique classes: 19\n",
            "\n",
            "Training XGBoost model...\n",
            "Training data shape: (2099976, 82)\n",
            "Number of classes: 27\n",
            "\n",
            "Evaluating model: XGBoost [Trained on 5 days] - Dataset A\n",
            "Test data shape: (10000, 82)\n",
            "Prediction time: 27.14 μs/sample\n",
            "XGBoost [Trained on 5 days] - Dataset A0.989600000.98937538104         104         0.98960000\n",
            "\n",
            "Classification Report:\n",
            "                                      precision    recall  f1-score   support\n",
            "\n",
            "                              BENIGN       1.00      1.00      1.00      5000\n",
            "                              Botnet       1.00      1.00      1.00         5\n",
            "                  Botnet - Attempted       1.00      1.00      1.00        37\n",
            "                                DDoS       1.00      1.00      1.00       923\n",
            "                       DoS GoldenEye       1.00      1.00      1.00        77\n",
            "                            DoS Hulk       1.00      1.00      1.00      1496\n",
            "                DoS Hulk - Attempted       1.00      1.00      1.00         1\n",
            "                    DoS Slowhttptest       1.00      1.00      1.00        14\n",
            "        DoS Slowhttptest - Attempted       1.00      1.00      1.00        29\n",
            "                       DoS Slowloris       1.00      1.00      1.00        34\n",
            "           DoS Slowloris - Attempted       1.00      1.00      1.00        12\n",
            "                         FTP-Patator       1.00      1.00      1.00        37\n",
            "                        Infiltration       1.00      1.00      1.00         1\n",
            "             Infiltration - Portscan       0.95      0.92      0.93       700\n",
            "                            Portscan       0.96      0.98      0.97      1581\n",
            "                         SSH-Patator       1.00      1.00      1.00        25\n",
            "            Web Attack - Brute Force       1.00      1.00      1.00         3\n",
            "Web Attack - Brute Force - Attempted       0.65      1.00      0.79        15\n",
            "        Web Attack - XSS - Attempted       1.00      0.20      0.33        10\n",
            "\n",
            "                            accuracy                           0.99     10000\n",
            "                           macro avg       0.98      0.95      0.95     10000\n",
            "                        weighted avg       0.99      0.99      0.99     10000\n",
            "\n",
            "Confusion matrix saved as 'confusion_matrix_xgboost_trained_on_5_days_-_dataset_a.png'\n",
            "Model saved as 'xgboost_5days.json'\n",
            "\n",
            "\n",
            "Scenario 2: Train on 5 days, test on Dataset B 10k\n",
            "--------------------------------------------------------------------------------\n",
            "Loaded dataset with shape: (10000, 91)\n",
            "\n",
            "Preprocessing dataset with shape: (10000, 91)\n",
            "Label distribution: 8 unique classes\n",
            "Top 5 classes: {'BENIGN': np.int64(5000), 'Portscan': np.int64(2401), 'DDoS': np.int64(1391), 'Infiltration - Portscan': np.int64(1107), 'Botnet - Attempted': np.int64(60)}\n",
            "NaN values before handling: 0\n",
            "NaN values after handling: 0\n",
            "Preprocessed dataset shape: (10000, 83)\n",
            "Number of unique classes: 8\n",
            "\n",
            "Evaluating model: XGBoost [Trained on 5 days] - Dataset B\n",
            "Test data shape: (10000, 82)\n",
            "Prediction time: 26.26 μs/sample\n",
            "XGBoost [Trained on 5 days] - Dataset B0.982000000.98194025180         180         0.98200000\n",
            "\n",
            "Classification Report:\n",
            "                                      precision    recall  f1-score   support\n",
            "\n",
            "                              BENIGN       1.00      1.00      1.00      5000\n",
            "                              Botnet       1.00      1.00      1.00        11\n",
            "                  Botnet - Attempted       1.00      1.00      1.00        60\n",
            "                                DDoS       1.00      1.00      1.00      1391\n",
            "             Infiltration - Portscan       0.93      0.91      0.92      1107\n",
            "                            Portscan       0.96      0.97      0.96      2401\n",
            "Web Attack - Brute Force - Attempted       0.79      0.83      0.81        18\n",
            "        Web Attack - XSS - Attempted       0.73      0.67      0.70        12\n",
            "\n",
            "                            accuracy                           0.98     10000\n",
            "                           macro avg       0.93      0.92      0.92     10000\n",
            "                        weighted avg       0.98      0.98      0.98     10000\n",
            "\n",
            "Confusion matrix saved as 'confusion_matrix_xgboost_trained_on_5_days_-_dataset_b.png'\n",
            "\n",
            "\n",
            "Scenario 3: Train on 3 days, test on Dataset B 10k\n",
            "--------------------------------------------------------------------------------\n",
            "Loading data from days: ['monday.csv', 'tuesday.csv', 'wednesday.csv']\n",
            "  - monday.csv: 371624 samples\n",
            "  - tuesday.csv: 322078 samples\n",
            "  - wednesday.csv: 496641 samples\n",
            "Combined dataset shape: (1190343, 91)\n",
            "\n",
            "Preprocessing dataset with shape: (1190343, 91)\n",
            "Label distribution: 14 unique classes\n",
            "Top 5 classes: {'BENIGN': np.int64(1005850), 'DoS Hulk': np.int64(158468), 'DoS GoldenEye': np.int64(7567), 'FTP-Patator': np.int64(3972), 'DoS Slowloris': np.int64(3859)}\n",
            "\n",
            "Label encoding mapping:\n",
            "  0: BENIGN\n",
            "  1: DoS GoldenEye\n",
            "  2: DoS GoldenEye - Attempted\n",
            "  3: DoS Hulk\n",
            "  4: DoS Hulk - Attempted\n",
            "  5: DoS Slowhttptest\n",
            "  6: DoS Slowhttptest - Attempted\n",
            "  7: DoS Slowloris\n",
            "  8: DoS Slowloris - Attempted\n",
            "  9: FTP-Patator\n",
            "  10: FTP-Patator - Attempted\n",
            "  11: Heartbleed\n",
            "  12: SSH-Patator\n",
            "  13: SSH-Patator - Attempted\n",
            "NaN values before handling: 0\n",
            "NaN values after handling: 0\n",
            "Preprocessed dataset shape: (1190343, 83)\n",
            "Number of unique classes: 14\n",
            "\n",
            "Preprocessing dataset with shape: (10000, 91)\n",
            "Label distribution: 8 unique classes\n",
            "Top 5 classes: {'BENIGN': np.int64(5000), 'Portscan': np.int64(2401), 'DDoS': np.int64(1391), 'Infiltration - Portscan': np.int64(1107), 'Botnet - Attempted': np.int64(60)}\n",
            "Found 7 unknown labels in test data\n",
            "NaN values before handling: 0\n",
            "NaN values after handling: 0\n",
            "Preprocessed dataset shape: (10000, 83)\n",
            "Number of unique classes: 2\n",
            "\n",
            "Training XGBoost model...\n",
            "Training data shape: (1190343, 82)\n",
            "Number of classes: 15\n",
            "\n",
            "Evaluating model: XGBoost [Trained on 3 days] - Dataset B\n",
            "Test data shape: (10000, 82)\n",
            "Prediction time: 14.30 μs/sample\n",
            "XGBoost [Trained on 3 days] - Dataset B0.500000000.368731565000        5000        0.50000000\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      BENIGN       0.58      1.00      0.74      5000\n",
            "     UNKNOWN       0.00      0.00      0.00      5000\n",
            "\n",
            "   micro avg       0.58      0.50      0.54     10000\n",
            "   macro avg       0.29      0.50      0.37     10000\n",
            "weighted avg       0.29      0.50      0.37     10000\n",
            "\n",
            "Confusion matrix saved as 'confusion_matrix_xgboost_trained_on_3_days_-_dataset_b.png'\n",
            "Model saved as 'xgboost_3days.json'\n",
            "\n",
            "\n",
            "Summary of All Scenarios\n",
            "====================================================================================================\n",
            "Model                              Accuracy    F1 Score    False PostiveFalse NegativeDetection   \n",
            "====================================================================================================\n",
            "\n",
            "Analysis complete!\n"
          ]
        }
      ]
    }
  ]
}